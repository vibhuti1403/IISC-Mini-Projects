{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFBl3DsqB3AE"
      },
      "source": [
        "# Advanced Certification Program in Computational Data Science\n",
        "\n",
        "##  A program by IISc and TalentSprint\n",
        "\n",
        "### Mini Project Notebook: Customer Churn Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maritime-miami"
      },
      "source": [
        "## Learning Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QEyzXLhmfky"
      },
      "source": [
        "At the end of the experiment, you will be able to :\n",
        "\n",
        "* find users that are going to churn in future\n",
        "* find what factors drive users to churn\n",
        "* perform EDA on the given churn data and prepare data for prediction task.\n",
        "* apply various machine learning algorithms and analyse the results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tddvYQAmmbP"
      },
      "source": [
        "## Information\n",
        "\n",
        "**Churn Analysis**\n",
        "\n",
        "Customer churn analysis refers to the customer attrition rate in a company. This analysis helps identify the cause of the churn and implement effective strategies for retention.\n",
        "\n",
        "\n",
        "Customer Churn is used to describe subscribers to a service who decide to discontinue their service for a certain time frame. Churn prediction consists of detecting which customers are likely to cancel a subscription to a service based on how they use the service.\n",
        "\n",
        "Businesses often have to invest substantial amounts attracting new clients, so every time a client leaves it represents a significant investment lost. Both time and effort then need to be channelled into replacing them. Being able to predict when a client is likely to leave and offer them incentives to stay can offer huge savings to a business.\n",
        "\n",
        "**Predicting customer churn with machine learning**\n",
        "\n",
        "As with any machine learning task, data science specialists first need data to work with. Depending on the goal, selected data is prepared, preprocessed, and transformed in a form suitable for building machine learning models. Finding the right methods to training machines, fine-tuning the models, and selecting the best performers is another significant part of the work. Once a model that makes predictions with the highest accuracy is chosen, it can be put into production.\n",
        "\n",
        "The overall scope of work data scientists carry out to build ML-powered systems capable to forecast customer attrition may look like the following:\n",
        "\n",
        "* Understanding a problem and final goal\n",
        "* Data collection\n",
        "* Data preparation and preprocessing\n",
        "* Modeling and testing\n",
        "* Model deployment and monitoring"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GGippmSmmbS"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "The dataset chosen for this task is customer churn dataset representing the trips of the users and drivers rating along with luxury cars used. Every row represents a separate customer. The data has a total of 50,000 customers.\n",
        "\n",
        "variables\tdescription\n",
        "* **city:**\tcity this user signed up in\n",
        "* **phone:**\tprimary device for this user\n",
        "* **signup_date:**\tdate of account registration; in the form `YYYYMMDD`\n",
        "* **last_trip_date:**\tthe last time this user completed a trip; in the form `YYYYMMDD`\n",
        "* **avg_dist:**\tthe average distance (in miles) per trip taken in the first 30 days after signup\n",
        "* **avg_rating_by_driver:**\tthe rider’s average rating over all of their trips\n",
        "* **avg_rating_of_driver:**\tthe rider’s average rating of their drivers over all of their trips\n",
        "* **surge_pct:**\tthe percent of trips taken with surge multiplier > 1\n",
        "* **avg_surge:**\tThe average surge multiplier over all of this user’s trips\n",
        "* **trips_in_first_30_days:**\tthe number of trips this user took in the first 30 days after signing up\n",
        "* **luxury_car_user:**\tTRUE if the user took a luxury car in their first 30 days; FALSE otherwise\n",
        "* **weekday_pct:**\tthe percent of the user’s trips occurring during a weekday\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1xnPizSmmbU"
      },
      "source": [
        "## Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cy34JDEcmmbU"
      },
      "source": [
        "Analyse and preprocess the data and build machine learning model to  predict Customer Churn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSkt1dXHmmbV"
      },
      "source": [
        "## Grading = 10 Points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "kX5ljpgMqkxh"
      },
      "outputs": [],
      "source": [
        "#@title Download Dataset\n",
        "!wget -qq https://cdn.iisc.talentsprint.com/CDS/MiniProjects/churn.csv\n",
        "print(\"Dataset downloaded successfully!!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "By9g1ScCmmb5"
      },
      "source": [
        "### Import required Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "poyI6Ahemmb7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn import metrics\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import ensemble\n",
        "from xgboost import XGBClassifier\n",
        "import warnings\n",
        "warnings.simplefilter('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4u7YxHHDxkjD"
      },
      "source": [
        "### Load the data and summarize (1 point)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgpRk3T3blHj"
      },
      "outputs": [],
      "source": [
        "# reading the .csv file\n",
        "data = pd.read_csv(\"/content/churn.csv\")\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgBPDaDxBL8e"
      },
      "source": [
        "#### Summarize the data\n",
        "* Explore the datatypes of the columns and correct\n",
        "* Identify the numerial, categorical and date columns\n",
        "* Identify the columns with missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ozac2mS4Bw8h"
      },
      "outputs": [],
      "source": [
        "#Available Information\n",
        "data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mk0TMAILBxrC"
      },
      "outputs": [],
      "source": [
        "#Lets first convert columns to their appropriate data types\n",
        "data.signup_date = data.signup_date.astype('datetime64')\n",
        "data.last_trip_date = data.last_trip_date.astype(\"datetime64\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCvdgPkuBmUY"
      },
      "source": [
        "#### Breakdown by months\n",
        "\n",
        "* Using the `last_trip_date` get the data for each month"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "maRQKMWseXSi"
      },
      "outputs": [],
      "source": [
        "data['last_trip_date'].max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ruKr7VslCF7Z"
      },
      "outputs": [],
      "source": [
        "#Lets see the breakdown by months\n",
        "last_trip_bd = data.groupby(data['last_trip_date'].dt.strftime('%B')).last_trip_date.count()\n",
        "cats = ['January', 'February', 'March', 'April','May','June', 'July', 'August','September', 'October', 'November', 'December']\n",
        "last_trip_bd.index = pd.CategoricalIndex(last_trip_bd.index, categories=cats, ordered=True)\n",
        "last_trip_bd = last_trip_bd.sort_index()\n",
        "last_trip_bd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yhh6W2DOCQ0_"
      },
      "source": [
        "Clearly, users who have used the app in July and June, are customers who are still loyal to the company. However, customers who last used the app before June (in May or before) have gone by without using the app for a considerable time. Lets mark them as inactive (or users who have churned)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2lMBZwIxrZr"
      },
      "source": [
        "### Data Preparation (Target variable - Churn) ( 2 points)\n",
        "\n",
        "Clearly, users who have used the app in July and June, are customers who are still loyal to the company. However, customers who last used the app before June (in May or before) have gone by without using the app for a considerable time. Lets mark them as inactive (or users who have churned).\n",
        "\n",
        "**Note:** Any user whose last trip with the company was before 1st June, 2014 is considered to be \"churned\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o140hAXaCN6x"
      },
      "outputs": [],
      "source": [
        "#Any user whose last trip with the company was before 1st June, 2014 is considered to be \"churned\".\n",
        "data[\"churned\"] = 1\n",
        "data[\"churned\"][data.last_trip_date >= \"2014-06-01\"] = 0\n",
        "data.churned = data.churned.astype(\"category\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTEAEUmgfLHV"
      },
      "outputs": [],
      "source": [
        "data[data[\"churned\"] == 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BUqIhxMnDO7"
      },
      "source": [
        "#### Handle the Duplicates\n",
        "\n",
        "Although, we dont have a unique customer ID for each customer, having all values similar looks highly unlikely for 2 customers. Find such rows in the data (customer having the same city, same phone, same signup_date, same last_trip_date looks highly unlikely) and drop.\n",
        "\n",
        "Hint: `drop_duplicates()`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WugGKNc4CTzr"
      },
      "outputs": [],
      "source": [
        "data[data.duplicated()].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bgGQwYQCcVr"
      },
      "source": [
        "Although, we dont have a unique customer ID for each customer, having all values similar looks highly unlikely for 2 customers. There are 8 such rows in the data (customer having the same city, same phone, same signup_date, same last_trip_date looks highly unlikely)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63W02YayCXVk"
      },
      "outputs": [],
      "source": [
        "clean_data = data.copy()\n",
        "clean_data.drop_duplicates(inplace = True)\n",
        "\n",
        "#We have a total of 49,992 customers\n",
        "clean_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NX37-N4DDx2"
      },
      "outputs": [],
      "source": [
        "#Since, we have used last_trip_date to create our target variable, we can drop the variable from further analysis\n",
        "clean_data.drop([\"last_trip_date\"],axis = 1,inplace = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2islBuvzhaU"
      },
      "source": [
        "#### Separate columns by data types\n",
        "\n",
        "* Identify the columns belongs various data types and separate them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYRKXy2VDHPw"
      },
      "outputs": [],
      "source": [
        "#Before moving further, lets separate our variables based on their types\n",
        "#Separating columns by data types\n",
        "def separate(df):\n",
        "    separated_cols = {\n",
        "        \"categorical\" : list(df.select_dtypes(include = [\"bool\",\"object\",\"category\"]).columns),\n",
        "        \"continuous\" : list(df.select_dtypes(include = [\"int64\",\"float64\"]).columns),\n",
        "        \"date\" : list(df.select_dtypes(include = [\"datetime\"]).columns)\n",
        "    }\n",
        "    return separated_cols\n",
        "\n",
        "separate(clean_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AX6AXyKXDl4k"
      },
      "source": [
        "#### Handle the null values\n",
        "\n",
        "* Identify and handle the null values and provide the justification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d38DMcfhEnLk"
      },
      "outputs": [],
      "source": [
        "# Missing Values\n",
        "clean_data.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5kRuvqdGtmi"
      },
      "source": [
        "Replacing Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "euKUcczRGifG"
      },
      "outputs": [],
      "source": [
        "clean_data[\"phone\"] = clean_data[\"phone\"].fillna(\"Other\")\n",
        "\n",
        "clean_data[\"phone\"].value_counts()\n",
        "\n",
        "#Before replacing values for rating variables, lets create a separate variable indicating that values have been replaced here!\n",
        "clean_data[\"rating_by_driver_replaced\"] = 0\n",
        "clean_data[\"rating_by_driver_replaced\"][clean_data.avg_rating_by_driver.isnull()] = 1\n",
        "clean_data[\"rating_of_driver_replaced\"] = 0\n",
        "clean_data[\"rating_of_driver_replaced\"][clean_data.avg_rating_of_driver.isnull()] = 1\n",
        "\n",
        "#Replacing ratings with median of the variable, since it is a highly skewed variable.\n",
        "clean_data[\"avg_rating_by_driver\"] = clean_data[\"avg_rating_by_driver\"].fillna(clean_data.avg_rating_by_driver.median())\n",
        "clean_data[\"avg_rating_of_driver\"] = clean_data[\"avg_rating_of_driver\"].fillna(clean_data.avg_rating_of_driver.median())\n",
        "\n",
        "clean_data.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfBpVbOdngqG"
      },
      "source": [
        "* Why does column \"phone\" have missing values? The customer needs a phone to use the app - Could be an different OS\n",
        "* Having missing values for ratings seem intuitive. Not all customers provide a rating to the drivers. Similar for Drivers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhW8-4zYnd35"
      },
      "source": [
        "#### Outliers Detection\n",
        "\n",
        "* Investigate outliers for every variable find the neccesary variables suitable for modelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rL0SNQqSFxU_"
      },
      "outputs": [],
      "source": [
        "clean_data[clean_data[\"avg_dist\"] > 50].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gV1FZFyLF-Vw"
      },
      "outputs": [],
      "source": [
        "clean_data[clean_data[\"avg_dist\"] > 50].head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDxoyaj4GFNO"
      },
      "source": [
        "Why are trips_in_first_30_days = 0, when avg_dist travelled by customer is higher than 0?\n",
        "\n",
        "* If customer did not take any trip after signing up (in first 30 days), then dist. travelled should be 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ET507gHqGAu_"
      },
      "outputs": [],
      "source": [
        "clean_data[(clean_data[\"avg_dist\"] > 0) & (clean_data[\"trips_in_first_30_days\"] == 0)].shape\n",
        "clean_data[(clean_data[\"avg_dist\"] > 0) & (clean_data[\"trips_in_first_30_days\"] == 0)].head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "068aGXdGGSeG"
      },
      "source": [
        "There are 15,000 such customers (thats 30% of observations)\n",
        "\n",
        "Looks like the variable has quality issues. We'll drop the variable from further analysis and not include it for modeling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0f84L7iGgRW"
      },
      "outputs": [],
      "source": [
        "clean_data.drop([\"trips_in_first_30_days\"],inplace = True, axis =1)\n",
        "\n",
        "#Lets look at remaining outliers\n",
        "clean_data[clean_data[\"avg_dist\"] == 0].shape\n",
        "clean_data[clean_data[\"avg_dist\"] == 0].head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZ6kEOaVxvS9"
      },
      "source": [
        "### Data Exploration & Analysis (1 point)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3KXARcBoqej"
      },
      "source": [
        "#### Univariate Analysis\n",
        "\n",
        "* Analyze each variable individually with appropriate plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7iy8hrgEt38"
      },
      "outputs": [],
      "source": [
        "#Churn distribution\n",
        "clean_data.churned.value_counts(normalize = True).plot(kind = \"bar\",title = \"Class distribution: Churned\")\n",
        "plt.xticks(np.arange(2),labels = [\"churned\",\"active\"])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_NEsH0NFVum"
      },
      "source": [
        "#### Categorical variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5SAeHoZSE-QU"
      },
      "outputs": [],
      "source": [
        "cat_cols = separate(clean_data)[\"categorical\"]\n",
        "cat_cols.remove(\"churned\")\n",
        "cat_cols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9m4maTSeFXjm"
      },
      "outputs": [],
      "source": [
        "fig, ax= plt.subplots(1,3, figsize = (15,5))\n",
        "for i, col in enumerate(cat_cols):\n",
        "    print(i)\n",
        "    sns.countplot(x = col,data = clean_data, ax = ax[i])\n",
        "    ax[i].set_title(\"Distribution:\"+ col.upper())\n",
        "clean_data[cat_cols].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o69uNO_WFoRb"
      },
      "source": [
        "#### Numerical Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZP0Zm0oFb0t"
      },
      "outputs": [],
      "source": [
        "cont_cols = separate(clean_data)[\"continuous\"]\n",
        "clean_data[cont_cols].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqjf9qIBFri_"
      },
      "outputs": [],
      "source": [
        "\n",
        "clean_data[cont_cols].mean().plot(kind = \"bar\")\n",
        "plt.title(\"Mean Distribution\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxgRLsx4FuAo"
      },
      "outputs": [],
      "source": [
        "clean_data[cont_cols].hist(figsize = (15,10),bins = 20)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rk50JcAwF0LC"
      },
      "source": [
        "**Insights**\n",
        "\n",
        "Almost all variables are skewed. We need to transform them.\n",
        "Customer/ Driver ratings can give us an insight into their behaviour and personality. We can create new features using the variable.\n",
        "Avg_surge has most obs. at 1 and surge_pct at 0. There could be some correlation here. Need further analysis.\n",
        "All outlier points need further investigation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIX81jKzo0gf"
      },
      "source": [
        "#### Bivariate Analysis\n",
        "\n",
        "* Identify relationships between variables with appropriate plot\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdcJRe4kGz1v"
      },
      "outputs": [],
      "source": [
        "fig,ax = plt.subplots(1,3,figsize = (15,5))\n",
        "for i in range(len(separate(clean_data)[\"categorical\"])-1):\n",
        "    temp = clean_data.groupby(separate(clean_data)[\"categorical\"][i])[\"churned\"].value_counts(normalize = True).unstack()\n",
        "    temp = temp[[1,0]]\n",
        "    temp.plot(kind = \"bar\",stacked = True,rot = 0,ax = ax[i])\n",
        "    ax[i].hlines(0.63,-10,100,linestyle = \"dashed\") #dashed line if the average customers churn rate\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzwMz-vmHBQH"
      },
      "source": [
        "**Insights**\n",
        "\n",
        "* City Astapor is experiencing a higher churn rate than average. Customers are unhappy in Astapor. King's Landing is managing the operations really well. A very low churn rate.\n",
        "* Android users are unhappy / churning - There can be various issues here for example - UI for the Android app is too complex/ difficult for customers or * customers experiencing other problems.\n",
        "* Customers taking a luxury car in first 30 days churn less. We should promote usage of luxury cars."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uEJ0273HhFr"
      },
      "source": [
        "### Feature Engineering (1 point)\n",
        "\n",
        "* Create a feature indicating android users face surge pricing more number of times\n",
        "* Create variable based on ratings indicating user is good/bad,  by grouping the average ratings of customers\n",
        "*  Create a variable to identify 3 groups of population by grouping the `weekday_pct`\n",
        "   - those  who dont ride during week\n",
        "   - those who ride only during week\n",
        "   - others\n",
        "   \n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DV3LIdS0G8jC"
      },
      "outputs": [],
      "source": [
        "#Android users facing surge\n",
        "clean_data[\"Android_user_facing_surge\"] = 'No'\n",
        "clean_data[\"Android_user_facing_surge\"][(clean_data[\"phone\"] == \"Android\") & (clean_data[\"surge_pct\"] != 0)] = \"Yes\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oVKBQ0XAISXh"
      },
      "outputs": [],
      "source": [
        "# Converting ratings into a categorical variable\n",
        "clean_data[\"customer_behaviour\"] = ''\n",
        "clean_data.customer_behaviour[clean_data.avg_rating_by_driver >= 4] = \"good\"\n",
        "clean_data.customer_behaviour[(clean_data.avg_rating_by_driver >= 3) & (clean_data.avg_rating_by_driver < 4)] = \"okay\"\n",
        "clean_data.customer_behaviour[clean_data.avg_rating_by_driver < 3] = \"bad\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgEyHwIgILRp"
      },
      "outputs": [],
      "source": [
        "#Weekday pct into groups\n",
        "clean_data[\"ride_during_week\"] = ''\n",
        "clean_data.ride_during_week[clean_data.weekday_pct == 0] = \"none\"\n",
        "clean_data.ride_during_week[clean_data.weekday_pct == 100] = \"all\"\n",
        "clean_data.ride_during_week[(clean_data.weekday_pct > 0) & (clean_data.weekday_pct < 100)] = \"some\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JpyY27j5IGU7"
      },
      "outputs": [],
      "source": [
        "#sanity check\n",
        "clean_data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3s1y-in5H_Tl"
      },
      "outputs": [],
      "source": [
        "#changing rating_by_driver_replaced dtype to object\n",
        "clean_data[\"rating_by_driver_replaced\"] = clean_data[\"rating_by_driver_replaced\"].astype(\"bool\")\n",
        "clean_data[\"rating_of_driver_replaced\"] = clean_data[\"rating_of_driver_replaced\"].astype(\"bool\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5JeC9CUaH6Zu"
      },
      "outputs": [],
      "source": [
        "#Removing signup_date\n",
        "clean_data.drop(\"signup_date\",axis =1, inplace = True)\n",
        "clean_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9UU_zLYHDK-"
      },
      "source": [
        "### Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BknHVoTo_YS"
      },
      "source": [
        "#### Plot the correlation heatmap and analyze"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fuiC5vc8Ii4E"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (15,10))\n",
        "sns.heatmap(clean_data.corr(),annot = True,linewidth = 0.2)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_NDFFHOIrOb"
      },
      "outputs": [],
      "source": [
        "clean_data.drop(\"avg_surge\",axis = 1,inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0tma68WIyiY"
      },
      "outputs": [],
      "source": [
        "#Converting categorical variables into dummy variables\n",
        "clean_data = pd.get_dummies(clean_data, drop_first = True,columns = [\"city\",\"phone\",\"Android_user_facing_surge\",\n",
        "                                                                     \"customer_behaviour\",\"ride_during_week\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZCuexdfJDhh"
      },
      "outputs": [],
      "source": [
        "#Before scaling, lets divide our data into training and testing splits\n",
        "features = [i for i in clean_data.columns if i != \"churned\"]\n",
        "target = [\"churned\"]\n",
        "X_train, X_test, y_train, y_test = train_test_split(clean_data[features], clean_data[target], test_size=0.2,random_state = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "903jVml6JTp4"
      },
      "source": [
        "#### Scaling the features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uy0inRWrJKxY"
      },
      "outputs": [],
      "source": [
        "scale = StandardScaler().fit(X_train)\n",
        "X_train = scale.transform(X_train)\n",
        "X_test = scale.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TK3leAgbJeas"
      },
      "source": [
        "#### PCA Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xeOXXyIJJVxC"
      },
      "outputs": [],
      "source": [
        "cor_mat1 = np.corrcoef(X_train.T)\n",
        "eig_vals, eig_vecs = np.linalg.eig(cor_mat1)\n",
        "# Looking at sorted eigenvalues\n",
        "rounded_eigs = [np.around(i,5) for i in eig_vals]\n",
        "sorted_eigs  = sorted(rounded_eigs, reverse = True)\n",
        "print('Eigenvalues in descending order:\\n',sorted_eigs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-FVsIp_x5Ii"
      },
      "source": [
        "### Train the Machine Learning models (3 points)\n",
        "\n",
        "\n",
        "* Apply all the ML models on the data\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lg_SoKgfpJJd"
      },
      "source": [
        "#### Evaluation Metrics - Model Comparison\n",
        "\n",
        "* Since, the target is to identify churning customers correctly, focus more on getting True Positive correct (High TPR). Let off False Positive errors (Customers we predicted will churn, but do not!) as they are not that important.\n",
        "\n",
        "* Also, lower the False Negative error (Customers we predicted will not churn, but they did churn!). In this case, we might lose these customers due to the error.\n",
        "\n",
        "**main target would be to - MAXIMIZE TRUE POSITIVES and MINIMIZE FALSE NEGATIVE ERRORS!**\n",
        "\n",
        "**Metrics:** Plot the ROC-AUC curve and confusion Matrix for all the models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJaB4QjBJhQe"
      },
      "outputs": [],
      "source": [
        "def cross_validation(model,xtrain,ytrain, scoretype, folds):\n",
        "    scores = cross_val_score(estimator = model,X= xtrain, y = ytrain,scoring = scoretype,cv = folds)\n",
        "    print(\"%s: %0.3f (+/- %0.2f)\" % (\"roc-auc\",scores.mean(),scores.std()))\n",
        "\n",
        "def roc_curve(X_test,y_test,model,model_name):\n",
        "    from sklearn import metrics\n",
        "    roc_auc = metrics.roc_auc_score(y_test,model.predict_proba(X_test)[:,1])\n",
        "    fpr,tpr,threshold = metrics.roc_curve(y_test,model.predict_proba(X_test)[:,1])\n",
        "    plt.figure()\n",
        "    plt.plot(fpr,tpr,label = \"Model:\" + model_name +(\" (AUC) = %0.2f\")%roc_auc)\n",
        "    plt.plot([0,1],[0,1],\"r--\")\n",
        "    plt.xlim(0,1)\n",
        "    plt.ylim(0,1)\n",
        "    plt.legend(loc = \"lower right\")\n",
        "    plt.xlabel(\"False Positive Rate\")\n",
        "    plt.ylabel(\"True Positive Rate\")\n",
        "    plt.title(\"ROC curve\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lz12xHlgKQuP"
      },
      "source": [
        "**Models**\n",
        "\n",
        "Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZWnakboKMT3"
      },
      "outputs": [],
      "source": [
        "log_model = LogisticRegression()\n",
        "log_model.fit(X_train, y_train)\n",
        "LogisticRegression()\n",
        "\n",
        "cross_validation(log_model,X_train,y_train,scoretype = \"roc_auc\",folds = 10)\n",
        "roc_curve(X_test,y_test,log_model,\"Logistic Regression\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ke0I_kdKeUw"
      },
      "source": [
        "Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37wLgP7qKZdB"
      },
      "outputs": [],
      "source": [
        "dtree = DecisionTreeClassifier(random_state = 42)\n",
        "dtree.fit(X_train,y_train)\n",
        "DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "cross_validation(dtree,X_train,y_train,scoretype = \"roc_auc\",folds = 10)\n",
        "roc_curve(X_test,y_test,dtree,\"Decision Tree\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_U0B16ARKm4Y"
      },
      "source": [
        "Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gb6ovzO2Kjdy"
      },
      "outputs": [],
      "source": [
        "rf = RandomForestClassifier(random_state = 42)\n",
        "rf.fit(X_train,y_train)\n",
        "RandomForestClassifier(random_state=42)\n",
        "\n",
        "cross_validation(rf,X_train,y_train,scoretype = \"roc_auc\",folds = 10)\n",
        "roc_curve(X_test,y_test,rf,\"Random Forest\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyhxFROLK0hH"
      },
      "source": [
        "Gradient Boosting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycLXwPpGKy3o"
      },
      "outputs": [],
      "source": [
        "gbm = ensemble.GradientBoostingClassifier(random_state = 30)\n",
        "gbm.fit(X_train,y_train)\n",
        "\n",
        "cross_validation(gbm,X_train,y_train,scoretype = \"roc_auc\",folds = 10)\n",
        "roc_curve(X_test,y_test,gbm,\"Gradient Boosting Classifier\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfWxhvg6K7tQ"
      },
      "source": [
        "AdaBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJw_svDmK52W"
      },
      "outputs": [],
      "source": [
        "ab = ensemble.AdaBoostClassifier(random_state = 30)\n",
        "ab.fit(X_train,y_train)\n",
        "\n",
        "cross_validation(ab,X_train,y_train,scoretype = \"roc_auc\",folds = 10)\n",
        "roc_curve(X_test,y_test,ab,\"Ada Boost Classifier\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5hxu1a7LCr8"
      },
      "outputs": [],
      "source": [
        "#combined results\n",
        "plt.figure(figsize = (15,7))\n",
        "\n",
        "roc_auc = metrics.roc_auc_score(y_test,log_model.predict_proba(X_test)[:,1])\n",
        "fpr,tpr,threshold = metrics.roc_curve(y_test,log_model.predict_proba(X_test)[:,1])\n",
        "plt.plot(fpr,tpr,label = \"Model: Logistic Regression\" + (\" (AUC) = %0.2f\")%roc_auc)\n",
        "\n",
        "roc_auc = metrics.roc_auc_score(y_test,dtree.predict_proba(X_test)[:,1])\n",
        "fpr,tpr,threshold = metrics.roc_curve(y_test,dtree.predict_proba(X_test)[:,1])\n",
        "plt.plot(fpr,tpr,label = \"Model: Decision Tree\" + (\" (AUC) = %0.2f\")%roc_auc)\n",
        "\n",
        "roc_auc = metrics.roc_auc_score(y_test,rf.predict_proba(X_test)[:,1])\n",
        "fpr,tpr,threshold = metrics.roc_curve(y_test,rf.predict_proba(X_test)[:,1])\n",
        "plt.plot(fpr,tpr,label = \"Model: Random Forest\" + (\" (AUC) = %0.2f\")%roc_auc)\n",
        "\n",
        "roc_auc = metrics.roc_auc_score(y_test,gbm.predict_proba(X_test)[:,1])\n",
        "fpr,tpr,threshold = metrics.roc_curve(y_test,gbm.predict_proba(X_test)[:,1])\n",
        "plt.plot(fpr,tpr,label = \"Model: Gradient Boosting\" + (\" (AUC) = %0.2f\")%roc_auc)\n",
        "\n",
        "roc_auc = metrics.roc_auc_score(y_test,ab.predict_proba(X_test)[:,1])\n",
        "fpr,tpr,threshold = metrics.roc_curve(y_test,ab.predict_proba(X_test)[:,1])\n",
        "plt.plot(fpr,tpr,label = \"Model: AdaBoost\" + (\" (AUC) = %0.2f\")%roc_auc)\n",
        "\n",
        "plt.plot([0,1],[0,1],\"r--\")\n",
        "plt.xlim(0,1)\n",
        "plt.ylim(0,1)\n",
        "plt.legend(loc = \"lower right\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC curve\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1d7U2nNpSj4"
      },
      "source": [
        "#### Model Optimization: Hyper-tuning parameters\n",
        "\n",
        "Short-list all the best working models, and hyper-tune their parameters and see whether we can improve the performance even further.\n",
        "\n",
        "There are two ways to hyper tune parameters:\n",
        "\n",
        "1. Grid Search\n",
        "2. Random Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GpTb5WJMLCI"
      },
      "outputs": [],
      "source": [
        "#Tuning learning-rate and number of trees - with change in learning rate, we are adjusting number of estimators as well\n",
        "param_grid = { 'learning_rate' : [0.15,0.1,0.05,0.01,0.005,0.001],\n",
        "              'n_estimators' :[100,250,500,750,1000,1250,1500,1750],\n",
        "              }\n",
        "grid = RandomizedSearchCV(estimator = ensemble.GradientBoostingClassifier(),param_distributions = param_grid,n_jobs =-1,scoring = \"roc_auc\")\n",
        "grid.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQHT2uTeMPcZ"
      },
      "outputs": [],
      "source": [
        "#Tuning max_depth\n",
        "param_grid = { 'max_depth' : list(np.linspace(0,10,6)),\n",
        "              }\n",
        "grid = GridSearchCV(estimator = ensemble.GradientBoostingClassifier(learning_rate = 0.05, n_estimators = 750 ),param_grid = param_grid,n_jobs =-1,cv =5,scoring = \"roc_auc\")\n",
        "grid.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XqT-9zh6MRgF"
      },
      "outputs": [],
      "source": [
        "print(grid.best_params_, grid.best_score_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "icvFuRh4Lqi_"
      },
      "outputs": [],
      "source": [
        "#Tuned Classifier\n",
        "gbm_tuned = ensemble.GradientBoostingClassifier(learning_rate =0.05 ,n_estimators =750 ,random_state = 42, max_depth =2)\n",
        "gbm_tuned.fit(X_train,y_train)\n",
        "roc_curve(X_test,y_test,gbm_tuned,\"Tuned Gradient Boosting Classifier\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRbpMpQ1x8_x"
      },
      "source": [
        "### Factors driving customers to churn (1 point)\n",
        "\n",
        "* Find the factors from the data which are causing customers to churn\n",
        "\n",
        "* Plot the features with a bar plot\n",
        "\n",
        "Hint: `model.feature_importances_`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwzbUy4nMjZW"
      },
      "outputs": [],
      "source": [
        "temp = pd.DataFrame({'features': features, 'importance': gbm_tuned.feature_importances_}).sort_values('importance',ascending = False)\n",
        "temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLZ4_XPUMp5-"
      },
      "outputs": [],
      "source": [
        "chart = sns.barplot(x = \"features\",y = \"importance\",data = temp)\n",
        "chart.set_xticklabels(chart.get_xticklabels(), rotation=90)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtgHIwjOyBMo"
      },
      "source": [
        "### Report Analysis\n",
        "\n",
        "* Find the city which experiencing a higher churn rate than average.\n",
        "\n",
        "* Which app users (Android/IOS) are unhappy / churning and why ?\n",
        "\n",
        "* Derive a insight on the Luxury cars and customers churned.\n",
        "\n",
        "* Discuss the overall factors causing the customers churn and reasons for poor ratings."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
