{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFBl3DsqB3AE"
      },
      "source": [
        "# Advanced Certification Program in Computational Data Science\n",
        "\n",
        "##  A program by IISc and TalentSprint\n",
        "\n",
        "### Mini Project Notebook: Video Classification using LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maritime-miami"
      },
      "source": [
        "## Learning Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95F1ym6qB8VU"
      },
      "source": [
        "At the end of the experiment, you will be able to :\n",
        "\n",
        "* extract frames out of a video\n",
        "* build the CNN model to extract features of the video frames\n",
        "* train the LSTM, GRU model to classify the video based on sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8aczZmzvXTc"
      },
      "source": [
        "## Information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0Ld7v8N6Z12"
      },
      "source": [
        "**Background:** The CNN LSTM architecture involves using Convolutional Neural Network (CNN) layers for feature extraction on input data combined with LSTMs to support sequence prediction.\n",
        "\n",
        "CNN LSTMs were developed for visual time series prediction problems and the application of generating textual descriptions from sequences of images (e.g. videos). Specifically, the problems of:\n",
        "\n",
        "\n",
        "\n",
        "*   Activity Recognition: Generating a textual description of an activity demonstrated in a sequence of images\n",
        "*   Image Description: Generating a textual description of a single image.\n",
        "*   Video Description: Generating a textual description of a sequence of images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwfDjPVOaTDV"
      },
      "source": [
        "**Applications:** Applications such as surveillance, video retrieval and\n",
        "human-computer interaction require methods for recognizing human actions in various scenarios. In the area of robotics, the tasks of\n",
        "autonomous navigation or social interaction could also take advantage of the knowledge extracted\n",
        "from live video recordings. Typical scenarios\n",
        "include scenes with cluttered, moving backgrounds, nonstationary camera, scale variations, individual variations in\n",
        "appearance and cloth of people, changes in light and view\n",
        "point and so forth. All of these conditions introduce challenging problems that can be addressed using deep learning (computer vision) models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgx1PkHfCDyJ"
      },
      "source": [
        "## Dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adc87eHGA_NY"
      },
      "source": [
        "**Dataset:** This dataset consists of labelled videos of 6 human actions (walking, jogging, running, boxing, hand waving and hand clapping) performed several times by 25 subjects in four different scenarios: outdoors s1, outdoors with scale variation s2, outdoors with different clothes s3 and indoors s4 as illustrated below.\n",
        "\n",
        "![img](http://www.nada.kth.se/cvap/actions/actions.gif)\n",
        "\n",
        "All sequences were taken over homogeneous backgrounds with a static camera with 25fps frame rate. The sequences were downsampled to the spatial resolution of 160x120 pixels and have a length of four seconds in average. In summary, there are 25x6x4=600 video files for each combination of 25 subjects, 6 actions and 4 scenarios. For this mini-project we have randomly selected 20% of the data as test set.\n",
        "\n",
        "Dataset source: https://www.csc.kth.se/cvap/actions/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WN7vlh6jB40i"
      },
      "source": [
        "**Methodology:**\n",
        "\n",
        "When performing image classification, we input an image to our CNN; Obtain the predictions from the CNN;\n",
        "Choose the label with the largest corresponding probability\n",
        "\n",
        "\n",
        "Since a video is just a series of image frames, in a video classification, we Loop over all frames in the video file;\n",
        "For each frame, pass the frame through the CNN; Classify each frame individually and independently of each other; Choose the label with the largest corresponding probability;\n",
        "Label the frame and write the output frame to disk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoLphNfQF_28"
      },
      "source": [
        "Refer this [Video Classification using Keras](https://medium.com/video-classification-using-keras-and-tensorflow/action-recognition-and-video-classification-using-keras-and-tensorflow-56badcbe5f77) for complete understanding and implementation example of video classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ih-oasWmdZul"
      },
      "source": [
        "## Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-18cjyCTCHE-"
      },
      "source": [
        "Train a CNN-LSTM based deep neural net to recognize the action being performed in a video."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "operating-latter"
      },
      "source": [
        "## Grading = 10 Points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "kX5ljpgMqkxh"
      },
      "outputs": [],
      "source": [
        "#@title Download Dataset\n",
        "!wget -qq https://cdn.iisc.talentsprint.com/CDS/MiniProjects/Actions.zip\n",
        "!unzip -qq Actions.zip\n",
        "print(\"Dataset downloaded successfully!!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abstract-stocks"
      },
      "source": [
        "### Import Required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2LgMamdMvHRv"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras import applications\n",
        "from keras import optimizers\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import *\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Input\n",
        "from keras.layers.pooling import GlobalAveragePooling2D\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.layers.wrappers import TimeDistributed\n",
        "from keras.optimizers import Adam,Nadam\n",
        "\n",
        "import os, glob\n",
        "import cv2\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qxK9Wb5tYxh"
      },
      "source": [
        "### Load the data and generate frames of video (2 points)\n",
        "\n",
        "Detecting an action is possible by analyzing a series of images (that we name “frames”) that are taken in time.\n",
        "\n",
        "Hint: Refer data preparation section in [keras_video_classification](https://keras.io/examples/vision/video_classification/)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Sqyn9UAsXy4"
      },
      "outputs": [],
      "source": [
        "data_dir = \"/content/Actions/train/\"\n",
        "img_height , img_width = 160, 120\n",
        "n_channels = 3\n",
        "seq_len = 20\n",
        "classes = os.listdir(data_dir)\n",
        "classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4Q8KnqcskT7"
      },
      "outputs": [],
      "source": [
        "# Creating frames from videos; selecting the frames from the middle of video\n",
        "def frames_extraction(video_path):\n",
        "    frames_list = []\n",
        "    vidObj = cv2.VideoCapture(video_path)\n",
        "    frames = vidObj.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "    fps = int(vidObj.get(cv2.CAP_PROP_FPS))\n",
        "    seconds = int(frames / fps)\n",
        "    frame_count = 0\n",
        "    frame_seq = 0\n",
        "    while frame_count < frames:\n",
        "        success, image = vidObj.read()\n",
        "        if success and frame_count == int(frames * frame_seq):\n",
        "            image = cv2.resize(image, (img_width, img_height))\n",
        "            frames_list.append(image)\n",
        "            frame_seq += 1 / seq_len\n",
        "        elif not success:\n",
        "            print(\"Defected frame\")\n",
        "            break\n",
        "        frame_count += 1\n",
        "\n",
        "    return frames_list, fps, seconds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ylxnlbTnwW7t"
      },
      "outputs": [],
      "source": [
        "# selecting first frames\n",
        "def frames_extraction(video_path):\n",
        "    frames_list = []\n",
        "    vidObj = cv2.VideoCapture(video_path)\n",
        "    frames = vidObj.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "    fps = int(vidObj.get(cv2.CAP_PROP_FPS))\n",
        "    seconds = int(frames / fps)\n",
        "    frame_count = 1\n",
        "    while frame_count <= seq_len:\n",
        "        success, image = vidObj.read()\n",
        "        if success:\n",
        "            image = cv2.resize(image, (img_width, img_height))\n",
        "            frames_list.append(image)\n",
        "        elif not success:\n",
        "            print(\"Defected frame\")\n",
        "            break\n",
        "        frame_count += 1\n",
        "\n",
        "    return frames_list, fps, seconds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifEhwYH9UmX2"
      },
      "outputs": [],
      "source": [
        "# testing with 1 sample of video\n",
        "vid = frames_extraction(\"/content/Actions/train/Handclapping/person01_handclapping_d3_uncomp.avi\")\n",
        "np.array(vid[0]).shape, vid[1], vid[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxQbHgx2vyO6"
      },
      "source": [
        "generating frames of all the videos and storing it in numpy array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BnSflzfusnB6"
      },
      "outputs": [],
      "source": [
        "def create_data(input_dir):\n",
        "    X, Y, fps_list, duration = [], [], [], []\n",
        "    classes_list = os.listdir(input_dir)\n",
        "    for c in classes_list:\n",
        "      print(c)\n",
        "      files_list = os.listdir(os.path.join(input_dir, c))\n",
        "      for f in files_list:\n",
        "        frames, fps, sec_s = frames_extraction(os.path.join(os.path.join(input_dir, c), f))\n",
        "        fps_list.append(fps)\n",
        "        duration.append(sec_s)\n",
        "        if len(frames) == seq_len:\n",
        "          X.append(frames)\n",
        "          Y.append(c)\n",
        "    X = np.asarray(X)\n",
        "    Y = np.asarray(Y)\n",
        "    return X, Y,fps_list, duration\n",
        "\n",
        "X, label, fps_list, duration = create_data(data_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9WoUnOCIJWKO"
      },
      "outputs": [],
      "source": [
        "X.shape, label.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1lCfOFyOjNSL"
      },
      "outputs": [],
      "source": [
        "# fps is common for all the videos\n",
        "set(fps_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LTW_j4qRjVAa"
      },
      "outputs": [],
      "source": [
        "# different duration for videos\n",
        "print(set(duration))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_S1e9FFJNGf2"
      },
      "outputs": [],
      "source": [
        "from keras.utils import np_utils\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "le.fit(label)\n",
        "y = le.transform(label)\n",
        "# one hot encoding Classes\n",
        "Y = np_utils.to_categorical(y)\n",
        "Y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRV1_NkUZj6K"
      },
      "source": [
        "#### Load Test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FovAqI6JtLtH"
      },
      "outputs": [],
      "source": [
        "test_data_dir = \"/content/Actions/test/\"\n",
        "Xtest, Ytest, _, _ = create_data(test_data_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rc-7BmLQGMOC"
      },
      "outputs": [],
      "source": [
        "ytest = le.transform(Ytest)\n",
        "\n",
        "Ytest = np_utils.to_categorical(ytest)\n",
        "Ytest.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlz370UzOl9g"
      },
      "outputs": [],
      "source": [
        "np.where(label==classes[3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-5U2piTgNQA"
      },
      "outputs": [],
      "source": [
        "plt.imshow(X[0][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7tx5ejuJYoI"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(seq_len * img_width / 100, len(classes)*img_height/100))\n",
        "columns = seq_len\n",
        "rows = 6\n",
        "counter = 1\n",
        "for i in list(range(len(label)))[::83]:\n",
        "  for j in range(seq_len):\n",
        "    fig.add_subplot(rows, columns, counter)\n",
        "    counter += 1\n",
        "    plt.imshow(X[i][j])\n",
        "    plt.title(label[i])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7SZdKfta3oXm"
      },
      "outputs": [],
      "source": [
        "# to play the video and to observe the ACTION in frames\n",
        "from moviepy.editor import *\n",
        "path=\"/content/Actions/train/Walking/person11_walking_d4_uncomp.avi\"\n",
        "\n",
        "clip=VideoFileClip(path)\n",
        "ipython_display(clip,width=280)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QleRUjGgtwTP"
      },
      "source": [
        "### Create the neural network\n",
        "\n",
        "We can build the model in several ways. We can use a well-known model that we inject in time distributed layer, or we can build our own.\n",
        "\n",
        "With custom ConvNet each input image of the sequence must pass to a convolutional network. The goal is to train that model for each frame and then decide the class to infer.\n",
        "\n",
        "* Use ConvNet and Time distributed to detect features.\n",
        "* Inject the Time distributed output to GRU or LSTM to treat time series.\n",
        "* Apply a DenseNet to take the decision, to classify."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVY2FVVDnATG"
      },
      "source": [
        "##### Build the ConvNet for the feature extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1kHGAqLUlB0"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Conv2D, BatchNormalization, MaxPool2D, GlobalMaxPool2D\n",
        "\n",
        "def build_convnet(shape=(160, 120, 3)):\n",
        "    model = keras.Sequential()\n",
        "    model.add(Conv2D(64, (3,3), input_shape=shape, activation='relu'))\n",
        "    model.add(Conv2D(64, (3,3), activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D())\n",
        "\n",
        "    model.add(Conv2D(128, (3,3), activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D())\n",
        "\n",
        "    model.add(Conv2D(512, (3,3), activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    # flatten...\n",
        "    # You can also use Flatten but GlobalMaxPool2D will reduce the number of outputs (getting only maximum values from the last convolution)\n",
        "    model.add(GlobalMaxPool2D())\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjHGfFWrt-cK"
      },
      "source": [
        "#### Build the Time Distributed model and DenseNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6H6LLLr1U-AP"
      },
      "outputs": [],
      "source": [
        "from keras.layers import TimeDistributed, GRU, Dense, Dropout\n",
        "\n",
        "def action_model(shape=(40, 160, 120, 3), nbout=6):\n",
        "    # Create our convnet with (160, 120, 3) input shape\n",
        "    convnet = build_convnet(shape[1:]) # Removes the '40' dimension index\n",
        "\n",
        "    # then create our final model\n",
        "    model = keras.Sequential()\n",
        "\n",
        "    # add the convnet with (5, 160, 120, 3) shape\n",
        "    model.add(TimeDistributed(convnet, input_shape=shape))\n",
        "\n",
        "    # here, you can also use GRU or LSTM\n",
        "    model.add(GRU(64))\n",
        "\n",
        "    # and finally, we make a decision network\n",
        "    model.add(Dense(1024, activation='relu'))\n",
        "    model.add(Dropout(.5))\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(.5))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(nbout, activation='softmax'))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcYlSojgwIrb"
      },
      "source": [
        "#### Setup the parameters and train the model with epochs in batch wise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAKAXQ1PVCe_"
      },
      "outputs": [],
      "source": [
        "INSHAPE = (seq_len,) + (img_height,img_width) + (n_channels,) # (5, 160, 120, 3)\n",
        "print(INSHAPE, len(classes))\n",
        "model = action_model(INSHAPE, len(classes))\n",
        "optimizer = keras.optimizers.Adam(0.00001)\n",
        "model.compile(optimizer, 'categorical_crossentropy', metrics=['acc'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ds9ECTK7tIIi"
      },
      "outputs": [],
      "source": [
        "model.fit(x = X, y = Y, validation_data=(Xtest, Ytest), epochs=25, batch_size = 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjuF85bqev6z"
      },
      "source": [
        "#### Model 2\n",
        "\n",
        "Model 2 with Time Distributed layers of CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-RXBvvyIjxW"
      },
      "outputs": [],
      "source": [
        "from keras.layers import TimeDistributed, Conv2D, Dense, MaxPooling2D, Flatten, LSTM, Dropout, BatchNormalization\n",
        "from keras import models\n",
        "model_cnlst = models.Sequential()\n",
        "model_cnlst.add(TimeDistributed(Conv2D(128, (3, 3), strides=(1,1),activation='relu'),\n",
        "                                input_shape=(seq_len, img_height, img_width, n_channels)))\n",
        "model_cnlst.add(TimeDistributed(Conv2D(64, (3, 3), strides=(1,1),activation='relu')))\n",
        "model_cnlst.add(TimeDistributed(MaxPooling2D(2,2)))\n",
        "model_cnlst.add(TimeDistributed(Conv2D(64, (3, 3), strides=(1,1),activation='relu')))\n",
        "model_cnlst.add(TimeDistributed(Conv2D(32, (3, 3), strides=(1,1),activation='relu')))\n",
        "model_cnlst.add(TimeDistributed(MaxPooling2D(2,2)))\n",
        "model_cnlst.add(TimeDistributed(BatchNormalization()))\n",
        "\n",
        "model_cnlst.add(TimeDistributed(Flatten()))\n",
        "model_cnlst.add(Dropout(0.2))\n",
        "\n",
        "model_cnlst.add(LSTM(32,return_sequences=False,dropout=0.2)) # used 32 units\n",
        "\n",
        "model_cnlst.add(Dense(64,activation='relu'))\n",
        "model_cnlst.add(Dense(32,activation='relu'))\n",
        "model_cnlst.add(Dropout(0.2))\n",
        "model_cnlst.add(Dense(len(classes), activation='softmax'))\n",
        "model_cnlst.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3Lu98UJIwoZ"
      },
      "outputs": [],
      "source": [
        "from keras import optimizers\n",
        "\n",
        "optimizer_new=optimizers.Adam(learning_rate=0.00001)\n",
        "model_cnlst.compile(optimizer=optimizer_new,loss='categorical_crossentropy',metrics=['acc'])\n",
        "# Training:\n",
        "history_new_cnlst=model_cnlst.fit(x = X, y = Y, validation_data=(Xtest, Ytest),\n",
        "                                  epochs=20, batch_size = 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gymldd5u_cul"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZQ2FDK0inR5"
      },
      "outputs": [],
      "source": [
        "plt.plot(history_new_cnlst.history['loss'],'r',label='training loss')\n",
        "plt.plot(history_new_cnlst.history['val_loss'],label='validation loss')\n",
        "plt.xlabel('# epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFrWlW4euu8S"
      },
      "source": [
        "### Use pre-trained model for feature extraction (4 points)\n",
        "\n",
        "To create a deep learning network for video classification:\n",
        "\n",
        "* Convert videos to sequences of feature vectors using a pretrained convolutional neural network, such as VGG16, to extract features from each frame.\n",
        "\n",
        "* Train an LSTM network on the sequences to predict the video labels.\n",
        "\n",
        "* Assemble a network that classifies videos directly by combining layers from both networks.\n",
        "\n",
        "Hint: [VGG-16 CNN and LSTM](https://riptutorial.com/keras/example/29812/vgg-16-cnn-and-lstm-for-video-classification)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJI62Ld9DnJo"
      },
      "outputs": [],
      "source": [
        "frames = seq_len\n",
        "rows = img_height\n",
        "columns = img_width"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YpJlYHcIBuyk"
      },
      "outputs": [],
      "source": [
        "# Functional API\n",
        "video = Input(shape=(seq_len,\n",
        "                     img_height,\n",
        "                     img_width,\n",
        "                     n_channels))\n",
        "cnn_base = VGG16(input_shape=(img_height,\n",
        "                              img_width,\n",
        "                              n_channels),\n",
        "                 weights=\"imagenet\",\n",
        "                 include_top=False)\n",
        "cnn_out = GlobalAveragePooling2D()(cnn_base.output)\n",
        "cnn = Model(cnn_base.input, cnn_out)\n",
        "cnn.trainable = False\n",
        "encoded_frames = TimeDistributed(cnn)(video)\n",
        "encoded_sequence = LSTM(256)(encoded_frames)\n",
        "hidden_layer = Dense(1024, activation=\"relu\")(encoded_sequence)\n",
        "outputs = Dense(len(classes), activation=\"softmax\")(hidden_layer)\n",
        "model = Model([video], outputs)\n",
        "optim = Adam(learning_rate=0.0002)\n",
        "model.compile(loss=\"categorical_crossentropy\",\n",
        "              optimizer=optim,\n",
        "              metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tyoxEljqYz-8"
      },
      "outputs": [],
      "source": [
        "vgg_history = model.fit(x= X, y=Y, validation_data=(Xtest, Ytest), batch_size=20, epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "st3UHHlIgx78"
      },
      "source": [
        "**Adding Dropout to reduce ovefitting**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zmh4fvuz-BKG"
      },
      "outputs": [],
      "source": [
        "# Sequential API\n",
        "cnn = keras.Sequential()\n",
        "cnn.add(VGG16(input_shape=(img_height, img_width, n_channels), weights=\"imagenet\", include_top=False))\n",
        "cnn.add(GlobalAveragePooling2D())\n",
        "cnn.trainable = False\n",
        "model = keras.Sequential()\n",
        "model.add(TimeDistributed(cnn))\n",
        "model.add(LSTM(512))\n",
        "model.add(Dense(4096, activation=\"relu\"))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(2048, activation=\"relu\"))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(512, activation=\"relu\"))\n",
        "model.add(Dense(len(classes), activation=\"softmax\"))\n",
        "optim = Adam(learning_rate=0.0002)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=optim, metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3962VF-M1yg"
      },
      "outputs": [],
      "source": [
        "vgg_history = model.fit(x= X, y=Y, validation_data=(Xtest, Ytest), batch_size=20, epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2SqfjYNnmjz"
      },
      "outputs": [],
      "source": [
        "plt.plot(vgg_history.history['loss'],'r',label='training loss')\n",
        "plt.plot(vgg_history.history['val_loss'],label='validation loss')\n",
        "plt.xlabel('# epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owOW-XyZb792"
      },
      "source": [
        "### Report Analysis\n",
        "\n",
        "* Report the video frames sequences used to classify the sequences correctly\n",
        "* Discuss the impact of the LSTM, GRU and TimeDistributed layers\n",
        "* Discuss about the model convergence using pre-trained and convnet"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
