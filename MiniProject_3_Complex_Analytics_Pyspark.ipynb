{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "correct-ecology",
      "metadata": {
        "id": "correct-ecology"
      },
      "source": [
        "# Advanced Certification Program in Computational Data Science\n",
        "## A program by IISc and TalentSprint\n",
        "### Mini-Project: Complex Analytics using Pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "terminal-strip",
      "metadata": {
        "id": "terminal-strip"
      },
      "source": [
        "## Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xz_UEWA3dSdx",
      "metadata": {
        "id": "xz_UEWA3dSdx"
      },
      "source": [
        "Perform complex analytics on a network intrusion dataset using Pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "decimal-replication",
      "metadata": {
        "id": "decimal-replication"
      },
      "source": [
        "## Learning Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GVjqB5s9dV7x",
      "metadata": {
        "id": "GVjqB5s9dV7x"
      },
      "source": [
        "At the end of the mini-project, you will be able to :\n",
        "\n",
        "* analyze the data using Pyspark\n",
        "* implement RDD based operations on the data\n",
        "* derive insights from the complex data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "quality-approval",
      "metadata": {
        "id": "quality-approval"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fUHkfZ4ydbLv",
      "metadata": {
        "id": "fUHkfZ4ydbLv"
      },
      "source": [
        "The dataset chosen for this mini-project is a [10% subset](https://www.kdd.org/kdd-cup/view/kdd-cup-1999/Data) of the **[KDD Cup 1999 dataset](http://kdd.ics.uci.edu/databases/kddcup99/task.html)** (Computer network intrusion detection). This is the dataset used for the Third International Knowledge Discovery and Data Mining Tools Competition. The competition task was to build a network intrusion detector, a predictive model capable of distinguishing between ``bad`` connections, called intrusions or attacks, and ``good`` normal connections. This database contains a standard set of data to be audited, which includes a wide variety of intrusions simulated in a military network environment."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-F_vtrjUdbLw",
      "metadata": {
        "id": "-F_vtrjUdbLw"
      },
      "source": [
        "## Information"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-G3MhteXdbLx",
      "metadata": {
        "id": "-G3MhteXdbLx"
      },
      "source": [
        "Since 1999, KDD’99 has been the most widely used data set for the evaluation of anomaly detection methods. This data set is prepared by S. J. Stolfo and is built based on the data captured in DARPA’98 IDS evaluation program. DARPA’98 is about 4 gigabytes of compressed raw (binary) tcpdump data of 7 weeks of network traffic, which can be processed into about 5 million connection\n",
        "records, each with about 100 bytes. KDD dataset consists of approximately 4,900,000 single connection vectors each of which contains 41 features and is labeled as either normal or an attack, with exactly one specific attack type. The simulated attacks fall into one of the following four categories:\n",
        "\n",
        "* Denial of Service Attack (DoS): making some computing or memory resources too busy so that they deny legitimate users access to these resources.\n",
        "* User to Root Attack (U2R): unauthorized access from a remote machine according to exploit machine's vulnerabilities.\n",
        "* Remote to Local Attack (R2L): unauthorized access to local super user (root) privileges using system's susceptibility.\n",
        "* Probing Attack: host and port scans as precursors to other attacks. An attacker scans a network to gather information or find known vulnerabilities.\n",
        "\n",
        "KDD’99 features can be classified into three groups:\n",
        "\n",
        "1) Basic features: this category encapsulates all the attributes that can be extracted from a TCP/IP connection. Most of these features leading to an implicit delay in detection.\n",
        "\n",
        "2) Traffic features: this category includes features that are computed with respect to a window interval and is divided into two groups:\n",
        "\n",
        "  * \"same host\" features\n",
        "\n",
        "  * \"same service\" features\n",
        "\n",
        "3) Content features: unlike most of the DoS and Probing attacks, the R2L and U2R attacks don’t have any intrusion frequent sequential patterns. This is because the DoS and Probing attacks involve many connections to some host(s) in a very short period of time, however the R2L and U2R attacks are embedded in the data portions of the packets, and normally involve only a single connection. To detect these kinds of attacks, we need some features to be able to look for suspicious behavior in the data portion, e.g., the number of failed login attempts. These features are called content features."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BewwTjZaJojg",
      "metadata": {
        "id": "BewwTjZaJojg"
      },
      "source": [
        "## Grading = 10 Points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lhdF5OB-InpC",
      "metadata": {
        "cellView": "form",
        "id": "lhdF5OB-InpC"
      },
      "outputs": [],
      "source": [
        "#@title Install packages and Download Dataset\n",
        "!pip -qq install pyspark\n",
        "# Download the data\n",
        "!wget -qq http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz\n",
        "# Download feature names\n",
        "!wget -qq https://kdd.ics.uci.edu/databases/kddcup99/kddcup.names\n",
        "print(\"Successfully Installed packages and downloaded datasets!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6VwMxTT0KObc",
      "metadata": {
        "id": "6VwMxTT0KObc"
      },
      "source": [
        "### Creating Spark Session and Load the data (1 point)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "italic-gross",
      "metadata": {
        "id": "italic-gross"
      },
      "source": [
        "#### Import required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "published-remove",
      "metadata": {
        "id": "published-remove"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import SQLContext\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.mllib.stat import Statistics\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from operator import add"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VMYq2CFZdoO2",
      "metadata": {
        "id": "VMYq2CFZdoO2"
      },
      "source": [
        "#### Create a Spark session\n",
        "\n",
        "Create a Spark session is a combined entry point of a Spark application, which came into implementation from Spark 2.0 (Instead of having various contexts, everything is encapsulated in a Spark session)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "confused-xerox",
      "metadata": {
        "id": "confused-xerox"
      },
      "outputs": [],
      "source": [
        "# Start spark session\n",
        "spark = SparkSession.builder.appName('Intrusion_Detector').getOrCreate()\n",
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eZsV9Hdzdr8q",
      "metadata": {
        "id": "eZsV9Hdzdr8q"
      },
      "source": [
        "#### Creating an RDD from a File\n",
        "\n",
        "The most common way of creating an RDD is to load it from a file. Notice that Spark's textFile can handle compressed files directly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "accredited-venture",
      "metadata": {
        "id": "accredited-venture"
      },
      "outputs": [],
      "source": [
        "#Accessing sparkContext from sparkSession instance.\n",
        "sc = spark.sparkContext"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pY0QylZEYTvg",
      "metadata": {
        "id": "pY0QylZEYTvg"
      },
      "source": [
        "Load the dataset and show the top 10 records\n",
        "\n",
        "Hint: sparkContext.textFile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wound-repository",
      "metadata": {
        "id": "wound-repository"
      },
      "outputs": [],
      "source": [
        "data_file = \"/content/kddcup.data_10_percent.gz\"\n",
        "raw_data = sc.textFile(data_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "threaded-arthur",
      "metadata": {
        "id": "threaded-arthur"
      },
      "outputs": [],
      "source": [
        "raw_data.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fancy-chapel",
      "metadata": {
        "id": "fancy-chapel"
      },
      "outputs": [],
      "source": [
        "raw_data.take(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EiuSFvsiK76j",
      "metadata": {
        "id": "EiuSFvsiK76j"
      },
      "source": [
        "### RDD Basic Operations (4 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BlDi4q2-dvrU",
      "metadata": {
        "id": "BlDi4q2-dvrU"
      },
      "source": [
        "#### Convert the data to CSV format (list of elements).\n",
        "\n",
        "To create a Dataframe using the RDD file, convert each row into a list by splitting with a comma (,)\n",
        "\n",
        "Hint: `map()` and `split()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "olive-serve",
      "metadata": {
        "id": "olive-serve"
      },
      "outputs": [],
      "source": [
        "csv_data = raw_data.map(lambda x: x.split(\",\"))\n",
        "head_rows = csv_data.take(5)\n",
        "head_rows[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "restricted-bonus",
      "metadata": {
        "id": "restricted-bonus"
      },
      "source": [
        "Count how many interactions are normal and attacked in the dataset.\n",
        "\n",
        "Hint: apply `filter` on each row, except the rows with 'normal.', all the remaining values are attacked."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "designed-malta",
      "metadata": {
        "id": "designed-malta"
      },
      "outputs": [],
      "source": [
        "normal_data = csv_data.filter(lambda x: x[41] == 'normal.')\n",
        "normal_data.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "F7zu2IwgSgVL",
      "metadata": {
        "id": "F7zu2IwgSgVL"
      },
      "outputs": [],
      "source": [
        "attack_data = csv_data.filter(lambda x: x[41] !='normal.')\n",
        "attack_data.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sImzkceDd5IT",
      "metadata": {
        "id": "sImzkceDd5IT"
      },
      "source": [
        "#### Protocol and Service combinations using Cartesian product\n",
        "\n",
        "We can compute the Cartesian product between two RDDs by using the Cartesian transformation. It returns all possible pairs of elements between two RDDs. In our case, we will use it to generate all the possible combinations between Service and Protocol in our network interactions.\n",
        "\n",
        "First of all, isolate each collection of values in two separate RDDs. For that use `distinct` on the CSV-parsed dataset. From the dataset description, we know that protocol is the second column and service is the third."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "automatic-turning",
      "metadata": {
        "id": "automatic-turning"
      },
      "outputs": [],
      "source": [
        "protocols = csv_data.map(lambda x: x[1]).distinct()\n",
        "protocols.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "military-disposition",
      "metadata": {
        "id": "military-disposition"
      },
      "outputs": [],
      "source": [
        "services = csv_data.map(lambda x: x[2]).distinct()\n",
        "services.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NNiqNM_Gd7_m",
      "metadata": {
        "id": "NNiqNM_Gd7_m"
      },
      "source": [
        "Now let's do the Cartesian product\n",
        "\n",
        "Hint: [cartesian](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.cartesian.html#:~:text=Return%20the%20Cartesian%20product%20of,and%20b%20is%20in%20other%20.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "focused-circulation",
      "metadata": {
        "id": "focused-circulation"
      },
      "outputs": [],
      "source": [
        "product = protocols.cartesian(services).collect()\n",
        "print(\"There are {} combinations of protocol X service\".format(len(product)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VGINGA9qd-zN",
      "metadata": {
        "id": "VGINGA9qd-zN"
      },
      "source": [
        "#### Inspecting interaction duration\n",
        "\n",
        "select the total duration of interactions for normal and attack intrusion types.\n",
        "* Use the above filtered normal and attacked data and convert the duration column to integer type using `map()`\n",
        "* get the sum of duration by applying `reduce` on both the data using add operator\n",
        "* find the mean of duration by dividing the sum with count\n",
        "\n",
        "Hint: [reduce()](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.RDD.reduce.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "requested-amateur",
      "metadata": {
        "id": "requested-amateur"
      },
      "outputs": [],
      "source": [
        "normal_duration_data = normal_data.map(lambda x: int(x[0]))\n",
        "attack_duration_data = attack_data.map(lambda x: int(x[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bulgarian-confidentiality",
      "metadata": {
        "id": "bulgarian-confidentiality"
      },
      "outputs": [],
      "source": [
        "total_normal_duration = normal_duration_data.reduce(add)\n",
        "total_attack_duration = attack_duration_data.reduce(add)\n",
        "\n",
        "print(\"Total duration for 'normal' interactions is {}\".format(total_normal_duration))\n",
        "print(\"Total duration for 'attack' interactions is {}\".format(total_attack_duration))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "directed-officer",
      "metadata": {
        "id": "directed-officer"
      },
      "outputs": [],
      "source": [
        "normal_count = normal_duration_data.count()\n",
        "attack_count = attack_duration_data.count()\n",
        "print(\"Mean duration for 'normal' interactions is {}\".format(total_normal_duration / float(normal_count)))\n",
        "print(\"Mean duration for 'attack' interactions is {}\".format(total_attack_duration / float(attack_count)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Vx9ev6IGeGN0",
      "metadata": {
        "id": "Vx9ev6IGeGN0"
      },
      "source": [
        "#### Data aggregation with key/value pair RDDs\n",
        "\n",
        "We can use all the transformations and actions available for normal RDDs with key/value pair RDDs. We just need to make the functions work with pair elements.\n",
        "\n",
        "* create a key/value pair of intrusion type and duration\n",
        "* calculate the total duration of each intrusion type using `reduceByKey()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "chicken-logging",
      "metadata": {
        "id": "chicken-logging"
      },
      "outputs": [],
      "source": [
        "key_value_duration = csv_data.map(lambda x: (x[41], float(x[0])))\n",
        "durations_by_key = key_value_duration.reduceByKey(add)\n",
        "\n",
        "durations_by_key.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "balanced-table",
      "metadata": {
        "id": "balanced-table"
      },
      "source": [
        "### Create a DataFrame with the header as features (2 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Y-S3RgOYdghm",
      "metadata": {
        "id": "Y-S3RgOYdghm"
      },
      "source": [
        "* Read the features (*kddcup.names*) and preprocess.\n",
        "\n",
        "    Hints:\n",
        "    - Each feature description appears row-wise in *kddcup.names*\n",
        "    - The first row consists of distinct values of intrusion_types\n",
        "    - Add or move the *intrusion_types* column name to the last, to align with the data.\n",
        "    - Each feature is represented as *feature_name*: *type*, remove *type* after colon (:)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "stylish-chance",
      "metadata": {
        "id": "stylish-chance"
      },
      "outputs": [],
      "source": [
        "names = sc.textFile(\"/content/kddcup.names\")\n",
        "firstRow = names.first()\n",
        "data = names.filter(lambda row:row != firstRow)\n",
        "data = data.map(lambda row: row.split(':')[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "supported-uzbekistan",
      "metadata": {
        "id": "supported-uzbekistan"
      },
      "outputs": [],
      "source": [
        "names_list = data.collect()\n",
        "names.count(), data.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "romance-cargo",
      "metadata": {
        "id": "romance-cargo"
      },
      "outputs": [],
      "source": [
        "names_list.append('intrusion_type')\n",
        "names_list"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XA-HDFEIdlwd",
      "metadata": {
        "id": "XA-HDFEIdlwd"
      },
      "source": [
        "* Create a dataframe with the data and headers as preprocessed feature names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "continuous-minneapolis",
      "metadata": {
        "id": "continuous-minneapolis"
      },
      "outputs": [],
      "source": [
        "sqlContext = SQLContext(sc)\n",
        "\n",
        "df = sqlContext.createDataFrame(csv_data).toDF(*names_list)\n",
        "df.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ndbtylfIebyU",
      "metadata": {
        "id": "ndbtylfIebyU"
      },
      "source": [
        "#### What is the count of each protocol type?\n",
        "\n",
        "Hint: apply `groupby` on protocol_type and count the records"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "unusual-arizona",
      "metadata": {
        "id": "unusual-arizona"
      },
      "outputs": [],
      "source": [
        "df.groupBy('protocol_type').count().orderBy('count', ascending=False).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "liable-edgar",
      "metadata": {
        "id": "liable-edgar"
      },
      "source": [
        "#### Register the DataFrame as a temporary table\n",
        "Using sql context, write a query to\n",
        "   * extract the label and their frequencies\n",
        "   * select the distinct protocol types with their count of transactions which are not normal\n",
        "   * select count of transactions in each protocol type that lasts more than 1 second (duration > 1000), with no data transfer from destination (dst_bytes == 0)\n",
        "\n",
        " Hint: `SQLContext.sql(query)`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "junior-advantage",
      "metadata": {
        "id": "junior-advantage"
      },
      "source": [
        "* extract the label and their frequencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "thirty-automation",
      "metadata": {
        "id": "thirty-automation"
      },
      "outputs": [],
      "source": [
        "df.registerTempTable(\"connections\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acknowledged-mixer",
      "metadata": {
        "id": "acknowledged-mixer"
      },
      "outputs": [],
      "source": [
        "labels = sqlContext.sql(\"\"\"SELECT intrusion_type, count(*) as freq FROM connections GROUP BY intrusion_type ORDER BY 2 DESC;\"\"\")\n",
        "labels.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "golden-corps",
      "metadata": {
        "id": "golden-corps"
      },
      "source": [
        "* select the distinct protocol types with their count of transactions which are not normal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "respiratory-parking",
      "metadata": {
        "id": "respiratory-parking"
      },
      "outputs": [],
      "source": [
        "attack_protocol = sqlContext.sql(\"\"\"\n",
        "                           SELECT\n",
        "                             protocol_type,\n",
        "                             COUNT(*) as freq\n",
        "                           FROM connections WHERE intrusion_type != 'normal.'\n",
        "                           GROUP BY protocol_type\n",
        "                           \"\"\")\n",
        "attack_protocol.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "intended-resource",
      "metadata": {
        "id": "intended-resource"
      },
      "source": [
        "* select count of transactions in each protocol type that lasts more than 1 second (duration > 1000), with no data transfer from destination (dst_bytes == 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "conventional-driving",
      "metadata": {
        "id": "conventional-driving"
      },
      "outputs": [],
      "source": [
        "# df.select(\"protocol_type\", \"duration\", \"dst_bytes\").filter(\n",
        "#     df.duration>1000).filter(df.dst_bytes==0).groupBy(\"protocol_type\").count().show()\n",
        "\n",
        "protocols_long_duration = sqlContext.sql(\"SELECT protocol_type, COUNT(*) FROM connections WHERE duration > 1000 AND dst_bytes==0 GROUP BY protocol_type\")\n",
        "protocols_long_duration.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15D8VgC5L7qI",
      "metadata": {
        "id": "15D8VgC5L7qI"
      },
      "source": [
        "### Find the highly correlated columns (2 points)\n",
        "\n",
        "* identify the columns which are not integer type and remove those columns\n",
        "* apply correlation function on the data (Hint: `Statistics.corr()`)\n",
        "* collect the names of the columns on which correlation is applied\n",
        "* create a dataframe with correlation matrix with index and columns as names\n",
        "* get the highly correlated features by considering a correlation value greater than 0.8\n",
        "\n",
        "    Hint: `np.triu()` , `pd.mask()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cooked-promise",
      "metadata": {
        "id": "cooked-promise"
      },
      "outputs": [],
      "source": [
        "def parse_interaction(line):\n",
        "    line_split = line.split(\",\")\n",
        "    # keep just numeric and logical values\n",
        "    symbolic_indexes = [1,2,3,41]\n",
        "    clean_line_split = [item for i,item in enumerate(line_split) if i not in symbolic_indexes]\n",
        "    return np.array([float(x) for x in clean_line_split])\n",
        "\n",
        "vector_data = raw_data.map(parse_interaction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "revolutionary-prerequisite",
      "metadata": {
        "id": "revolutionary-prerequisite"
      },
      "outputs": [],
      "source": [
        "correlation_matrix = Statistics.corr(vector_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "polish-claim",
      "metadata": {
        "id": "polish-claim"
      },
      "outputs": [],
      "source": [
        "# eliminating names of columns having string type\n",
        "numeric_names = [name for i,name in enumerate(names_list) if i not in [1,2,3,41]]\n",
        "numeric_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "european-aaron",
      "metadata": {
        "id": "european-aaron"
      },
      "outputs": [],
      "source": [
        "# creating a dataframe with correlation matrix\n",
        "corr_df = pd.DataFrame(correlation_matrix, index=numeric_names, columns=numeric_names)\n",
        "corr_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "skilled-domestic",
      "metadata": {
        "id": "skilled-domestic"
      },
      "outputs": [],
      "source": [
        "#  fill the null values with 0\n",
        "corr_df.fillna(0,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "approved-dublin",
      "metadata": {
        "id": "approved-dublin"
      },
      "outputs": [],
      "source": [
        "# Set Up Mask To Hide Upper Triangle\n",
        "mask = np.triu(np.ones_like(corr_df, dtype=bool))\n",
        "tri_df = corr_df.mask(mask)\n",
        "\n",
        "# Finding features with correlation value more than specified threshold value (bar=0.9)\n",
        "highly_cor_col = [col for col in tri_df.columns if any(tri_df[col] > 0.8 )]\n",
        "print(\"length of highly correlated columns\",highly_cor_col)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NnKVGdkrLJiy",
      "metadata": {
        "id": "NnKVGdkrLJiy"
      },
      "source": [
        "### Analysis report (1 points)\n",
        "\n",
        "* Find the ratio of attacked transactions vs normal transactions\n",
        "\n",
        "    Hint: encode instrusion_type column by replacing normal with 1 and all other with 0\n",
        "\n",
        "* Describe the statistics of attacked and normal transactions\n",
        "    \n",
        "    Hint: Min, Max, Mean\n",
        "    \n",
        "* Select any two features that influence the intrusion_type and visualize the scatter plot to see the separation of normal and attacked"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "greatest-pressing",
      "metadata": {
        "id": "greatest-pressing"
      },
      "outputs": [],
      "source": [
        "label_encode = udf(lambda x: 0 if x == 'normal.' else 1)\n",
        "df = df.withColumn('label',label_encode(df['intrusion_type']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dimensional-inspiration",
      "metadata": {
        "id": "dimensional-inspiration"
      },
      "outputs": [],
      "source": [
        "fea_label = highly_cor_col\n",
        "fea_label.append('label')\n",
        "fea_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "enormous-helmet",
      "metadata": {
        "id": "enormous-helmet"
      },
      "outputs": [],
      "source": [
        "data1 = df.select(fea_label)\n",
        "data1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "productive-brown",
      "metadata": {
        "id": "productive-brown"
      },
      "outputs": [],
      "source": [
        "# convert to Pandas and fill the null values\n",
        "df2 = data1.toPandas()\n",
        "df2.fillna(0,inplace=True)\n",
        "df2.label = df2.label.astype('int')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "clinical-melissa",
      "metadata": {
        "id": "clinical-melissa"
      },
      "outputs": [],
      "source": [
        "df2_normal = df2[df2['label']==0]\n",
        "df2_attacked = df2[df2['label']==1]\n",
        "# Ratio of attacked vs normal\n",
        "len(df2_attacked) / len(df2) , len(df2_normal)/len(df2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "varying-arkansas",
      "metadata": {
        "id": "varying-arkansas"
      },
      "outputs": [],
      "source": [
        "# describe\n",
        "df2.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cooperative-capacity",
      "metadata": {
        "id": "cooperative-capacity"
      },
      "outputs": [],
      "source": [
        "# compare the min, max, mean of attacked and normal\n",
        "df_stats = pd.DataFrame([df2_attacked.max(),df2_attacked.min(), df2_normal.max(), df2_normal.min()])\n",
        "df_stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "faced-darkness",
      "metadata": {
        "id": "faced-darkness"
      },
      "outputs": [],
      "source": [
        "# Scatter plot\n",
        "df2.plot(kind=\"scatter\", x=\"srv_count\", y=\"count\", label=\"label\",c= 'label',cmap='BrBG_r')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-Y3O2WSiiGzo",
      "metadata": {
        "id": "-Y3O2WSiiGzo"
      },
      "source": [
        "---------------------------------------- (or)  --------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mNjjIdd_ht5x",
      "metadata": {
        "id": "mNjjIdd_ht5x"
      },
      "outputs": [],
      "source": [
        "plt.scatter(df2['srv_count'],df2['count'],c=df2['label'])\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
