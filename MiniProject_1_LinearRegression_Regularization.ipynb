{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "chief-journalist",
      "metadata": {
        "id": "chief-journalist"
      },
      "source": [
        "# Advanced Certification Program in Computational Data Science\n",
        "## A program by IISc and TalentSprint\n",
        "### Mini-Project: Linear Regression with Regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "arabic-shaft",
      "metadata": {
        "id": "arabic-shaft"
      },
      "source": [
        "## Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "prospective-thread",
      "metadata": {
        "id": "prospective-thread"
      },
      "source": [
        "Predict the bike-sharing counts per hour based on features including weather, day, time, humidity, wind speed, season e.t.c."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "organic-christmas",
      "metadata": {
        "id": "organic-christmas"
      },
      "source": [
        "## Learning Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "phantom-begin",
      "metadata": {
        "id": "phantom-begin"
      },
      "source": [
        "At the end of the mini-project, you will be able to :\n",
        "\n",
        "* perform data exploration and visualization\n",
        "* implement linear regression using sklearn and optimization\n",
        "* apply regularization on regression using lasso and ridge techniques\n",
        "* calculate and compare the MSE value of each regression technique\n",
        "* analyze the features that are best contributing to the target"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "received-quilt",
      "metadata": {
        "id": "received-quilt"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Lih66z8Vm2lD",
      "metadata": {
        "id": "Lih66z8Vm2lD"
      },
      "source": [
        "The dataset chosen for this mini-project is [Bike Sharing Dataset](https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset).  This dataset contains the hourly and daily count of rental bikes between the years 2011 and 2012 in the capital bike share system with the corresponding weather and seasonal information. This dataset consists of 17389 instances of each 16 features.\n",
        "\n",
        "Bike sharing systems are a new generation of traditional bike rentals where the whole process from membership, rental and return has become automatic. Through these systems, the user can easily rent a bike from a particular position and return to another position. Currently, there are about over 500 bike-sharing programs around the world which is composed of over 500 thousand bicycles. Today, there exists great interest in these systems due to their important role in traffic, environmental and health issues.\n",
        "\n",
        "Apart from interesting real world applications of bike sharing systems, the characteristics of data being generated by these systems make them attractive for the research. As opposed to other transport services such as bus or subway, the duration of travel, departure and arrival position are explicitly recorded in these systems. This feature turns bike sharing system into a virtual sensor network that can be used for sensing mobility in the city. Hence, it is expected that the most important events in the city could be detected via monitoring these data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ISKMyTABbfp",
      "metadata": {
        "id": "5ISKMyTABbfp"
      },
      "source": [
        "<img src=\"https://s26551.pcdn.co/wp-content/uploads/2012/02/resize-va-sq-bikeshare.jpg\" alt=\"drawing\" width=\"400\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "binary-evening",
      "metadata": {
        "id": "binary-evening"
      },
      "source": [
        "### Data Fields\n",
        "\n",
        "* dteday - hourly date\n",
        "* season - 1 = spring, 2 = summer, 3 = fall, 4 = winter\n",
        "* hr - hour\n",
        "* holiday - whether the day is considered a holiday\n",
        "* workingday - whether the day is neither a weekend nor holiday\n",
        "* weathersit -<br>\n",
        "    1 - Clear, Few clouds, Partly cloudy, Partly cloudy <br>\n",
        "    2 - Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist<br>\n",
        "    3 - Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds<br>\n",
        "    4 - Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog<br>   \n",
        "* temp - temperature in Celsius\n",
        "* atemp - \"feels like\" temperature in Celsius\n",
        "* humidity - relative humidity\n",
        "* windspeed - wind speed\n",
        "* casual - number of non-registered user rentals initiated\n",
        "* registered - number of registered user rentals initiated\n",
        "* cnt - number of total rentals"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "perfect-fields",
      "metadata": {
        "id": "perfect-fields"
      },
      "source": [
        "## Information"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "quick-gasoline",
      "metadata": {
        "id": "quick-gasoline"
      },
      "source": [
        "**Regularization:** It is a form of regression, that constrains or regularizes or shrinks the coefficient estimates towards zero. In other words, this technique discourages learning a more complex or flexible model, to avoid the risk of overfitting. A simple relation for linear regression looks like this.\n",
        "\n",
        "$Y ≈ β_0 + β_1 X_1 + β_2 X_2 + …+ β_p X_p$\n",
        "\n",
        " Here $Y$ represents the learned relation and $β$ represents the coefficient estimates for different variables or predictors(X).\n",
        "\n",
        " If there is noise in the training data, then the estimated coefficients won’t generalize well to the future data. This is where regularization comes in and shrinks or regularizes these learned estimates towards zero.\n",
        "\n",
        "Below are the Regularization techniques:\n",
        "\n",
        " * Ridge Regression\n",
        " * Lasso Regression\n",
        " * Elasticnet Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "declared-battle",
      "metadata": {
        "id": "declared-battle"
      },
      "source": [
        "## Grading = 10 Points"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "correct-spectrum",
      "metadata": {
        "id": "correct-spectrum"
      },
      "source": [
        "### Download the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "virtual-ownership",
      "metadata": {
        "id": "virtual-ownership"
      },
      "outputs": [],
      "source": [
        "!wget -qq https://cdn.iisc.talentsprint.com/CDS/MiniProjects/Bike_Sharing_Dataset.zip\n",
        "!unzip -qq Bike_Sharing_Dataset.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cross-survivor",
      "metadata": {
        "id": "cross-survivor"
      },
      "source": [
        "#### Importing Necessary Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ethical-essence",
      "metadata": {
        "id": "ethical-essence"
      },
      "outputs": [],
      "source": [
        "# Loading the Required Packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import linear_model\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.metrics import r2_score"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "through-scotland",
      "metadata": {
        "id": "through-scotland"
      },
      "source": [
        "### Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "comic-consolidation",
      "metadata": {
        "id": "comic-consolidation"
      },
      "outputs": [],
      "source": [
        "# Reading Our Dataset\n",
        "bikeshare = pd.read_csv('hour.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ordered-overall",
      "metadata": {
        "id": "ordered-overall"
      },
      "source": [
        "first five rows of dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "exterior-committee",
      "metadata": {
        "id": "exterior-committee"
      },
      "outputs": [],
      "source": [
        "bikeshare.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sharp-shelter",
      "metadata": {
        "id": "sharp-shelter"
      },
      "outputs": [],
      "source": [
        "bikeshare.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "opponent-block",
      "metadata": {
        "id": "opponent-block"
      },
      "source": [
        "### Task flow with respect to feature processing and model training\n",
        "\n",
        "* Identify continuous features\n",
        "\n",
        "* Identify categorical features\n",
        "\n",
        "* Apply scaling on continuous features\n",
        "\n",
        "* Apply one-hot encoding on categorical features\n",
        "\n",
        "* Create features by concatenating all one hot encoded features and scaled features except target variables\n",
        "\n",
        "* Find the coefficients of the features using normal equation and find the cost (error)\n",
        "\n",
        "* Apply batch gradient descent technique by taking one target variable (cnt) and find the best coefficients\n",
        "\n",
        "* Apply SGD Regressor using sklearn\n",
        "\n",
        "* Apply linear regression using sklearn by taking two target variables (casual, registered)\n",
        "\n",
        "* Apply Lasso, Ridge, Elasticnet Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "magnetic-penny",
      "metadata": {
        "id": "magnetic-penny"
      },
      "source": [
        "### EDA &  Visualization ( 2 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "constitutional-techno",
      "metadata": {
        "id": "constitutional-techno"
      },
      "source": [
        "#### Visualize the hour (hr) column with an appropriate plot and find the busy hours of bike sharing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uniform-comfort",
      "metadata": {
        "id": "uniform-comfort"
      },
      "outputs": [],
      "source": [
        "bikeshare.groupby('hr').sum('cnt')['cnt'].plot.bar()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "flexible-export",
      "metadata": {
        "id": "flexible-export"
      },
      "source": [
        "#### Visualize the distribution of count, casual and registered variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "manufactured-introduction",
      "metadata": {
        "id": "manufactured-introduction"
      },
      "outputs": [],
      "source": [
        "# distribution of casual\n",
        "sns.distplot(bikeshare.casual);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "powerful-involvement",
      "metadata": {
        "id": "powerful-involvement"
      },
      "outputs": [],
      "source": [
        "# distribution of registered\n",
        "sns.distplot(bikeshare.registered);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "inside-consideration",
      "metadata": {
        "id": "inside-consideration"
      },
      "outputs": [],
      "source": [
        "# distribution of count\n",
        "sns.distplot(bikeshare.cnt);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "twelve-burton",
      "metadata": {
        "id": "twelve-burton"
      },
      "source": [
        "#### Describe the relation of weekday, holiday and working day"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afraid-proof",
      "metadata": {
        "id": "afraid-proof"
      },
      "outputs": [],
      "source": [
        "# Working days from 1-5 (mon-fri)\n",
        "bikeshare[bikeshare.workingday==1].weekday.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "outdoor-attack",
      "metadata": {
        "id": "outdoor-attack"
      },
      "outputs": [],
      "source": [
        "# Holiday possible on working days\n",
        "bikeshare[bikeshare.holiday==1].weekday.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "textile-breach",
      "metadata": {
        "id": "textile-breach"
      },
      "outputs": [],
      "source": [
        "# Not a holiday, not a working day (Sun, Sat)\n",
        "bikeshare[(bikeshare.holiday==0) & (bikeshare.workingday==0)].weekday.unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "thrown-allowance",
      "metadata": {
        "id": "thrown-allowance"
      },
      "source": [
        "#### Visualize the monthly wise count of both casual and registered for the year 2011 and 2012 separately.\n",
        "\n",
        "Hint: Stacked barchart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "collective-spanking",
      "metadata": {
        "id": "collective-spanking"
      },
      "outputs": [],
      "source": [
        "# stacked bar chart for year 2011\n",
        "bikeshare[bikeshare.yr==0].groupby('mnth').sum(['casual','registered'])[['casual','registered']].plot.bar(stacked=True);\n",
        "plt.title(\"Casual and Registered in 2011\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "joined-court",
      "metadata": {
        "id": "joined-court"
      },
      "outputs": [],
      "source": [
        "# stacked bar chart for year 2012\n",
        "bikeshare[bikeshare.yr==1].groupby('mnth').sum(['casual','registered'])[['casual','registered']].plot.bar(stacked=True)\n",
        "plt.title(\"Casual and Registered in 2012\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fifty-driver",
      "metadata": {
        "id": "fifty-driver"
      },
      "source": [
        "#### Analyze the correlation between features with heatmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "instant-coalition",
      "metadata": {
        "id": "instant-coalition"
      },
      "outputs": [],
      "source": [
        "sns.heatmap(bikeshare.iloc[:,:].corr(), cmap='RdBu')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pursuant-diary",
      "metadata": {
        "id": "pursuant-diary"
      },
      "source": [
        "#### Visualize the box plot of casual and registered variables to check the outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "stainless-robert",
      "metadata": {
        "id": "stainless-robert"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(nrows=1,ncols=2)\n",
        "sns.boxplot(data=bikeshare,y=\"casual\",orient=\"v\",ax=axes[0])\n",
        "sns.boxplot(data=bikeshare,y=\"registered\",orient=\"v\",ax=axes[1])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "comparative-heritage",
      "metadata": {
        "id": "comparative-heritage"
      },
      "source": [
        "### Pre-processing and Data Engineering (1 point)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "massive-scotland",
      "metadata": {
        "id": "massive-scotland"
      },
      "source": [
        "#### Drop unwanted columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "appreciated-lawyer",
      "metadata": {
        "id": "appreciated-lawyer"
      },
      "outputs": [],
      "source": [
        "bikeshare1 = bikeshare.drop(['instant', 'dteday'], axis = 1)\n",
        "bikeshare1.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nASeXE_7JC0L",
      "metadata": {
        "id": "nASeXE_7JC0L"
      },
      "source": [
        "#### Identify categorical and continuous variables\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "available-jersey",
      "metadata": {
        "id": "available-jersey"
      },
      "outputs": [],
      "source": [
        "# Identifying categorical and continuous variables\n",
        "cont_features = ['temp','atemp','hum','windspeed'] #,'casual','registered','cnt']\n",
        "categorical_features = ['season', 'yr', 'mnth','hr','holiday','weekday','weathersit']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xWpvuYRvnYB_",
      "metadata": {
        "id": "xWpvuYRvnYB_"
      },
      "source": [
        "#### Feature scaling\n",
        "\n",
        "Feature scaling is essential for machine learning algorithms, the range of all features should be normalized so that each feature contributes approximately proportionately to the final distance. Apply scaling on the continuous variables on the given data.\n",
        "\n",
        "Hint: `MinMaxScaler` or `StandardScaler`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "deadly-leisure",
      "metadata": {
        "id": "deadly-leisure"
      },
      "outputs": [],
      "source": [
        "std_scaler = StandardScaler()\n",
        "scaled_data = pd.DataFrame(std_scaler.fit_transform(bikeshare1[cont_features]), columns = cont_features)\n",
        "scaled_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "revised-reaction",
      "metadata": {
        "id": "revised-reaction"
      },
      "outputs": [],
      "source": [
        "# scaled features + categorical in one dataframe\n",
        "scaled_data\n",
        "for i in categorical_features:\n",
        "    scaled_data[i] = bikeshare1[i].values\n",
        "scaled_data.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fN8iuonSna3j",
      "metadata": {
        "id": "fN8iuonSna3j"
      },
      "source": [
        "#### Apply one-hot encode on the categorical data\n",
        "\n",
        "One-hot encoding is applied on the categorical variables, which should not have a different weight or order attached to them, it is presumed that all categorical variables have equivalent \"values\". This means that you cannot simply order them from zero to the number of categories as this would imply that the earlier categories have less \"value\" than later categories.\n",
        "\n",
        "Hint: `sklearn.preprocessing.OneHotEncoder`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "soviet-stockholm",
      "metadata": {
        "id": "soviet-stockholm"
      },
      "outputs": [],
      "source": [
        "onehot = OneHotEncoder()\n",
        "onehot_encoded = onehot.fit_transform(scaled_data[categorical_features]).toarray()\n",
        "onehot_encoded.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oBl2yT6Fj6Fk",
      "metadata": {
        "id": "oBl2yT6Fj6Fk"
      },
      "outputs": [],
      "source": [
        "onehot_encoded"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "straight-teens",
      "metadata": {
        "id": "straight-teens"
      },
      "source": [
        "#### Specify features and targets after applying scaling and one-hot encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "civic-private",
      "metadata": {
        "id": "civic-private"
      },
      "outputs": [],
      "source": [
        "features = np.concatenate((scaled_data[['temp','atemp','hum','windspeed']].values, onehot_encoded), axis=1)\n",
        "#features = scaled_data.values\n",
        "features.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cultural-beijing",
      "metadata": {
        "id": "cultural-beijing"
      },
      "outputs": [],
      "source": [
        "scaled_target = bikeshare1[['casual','registered','cnt']]\n",
        "scaled_target.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "apparent-restoration",
      "metadata": {
        "id": "apparent-restoration"
      },
      "source": [
        "### Implement the linear regression by finding the coefficients using below approaches (3 points)\n",
        "\n",
        "* Find the coefficients using Normal equation and find the error\n",
        "\n",
        "* Implement batch gradient descent\n",
        "\n",
        "* SGD Regressor from sklearn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "involved-shame",
      "metadata": {
        "id": "involved-shame"
      },
      "source": [
        "#### Select the features and target and split the dataset\n",
        "\n",
        "As there are 3 target variables, choose the count (`cnt`) variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "excess-interview",
      "metadata": {
        "id": "excess-interview"
      },
      "outputs": [],
      "source": [
        "target1 = bikeshare1[['cnt']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "chinese-sperm",
      "metadata": {
        "id": "chinese-sperm"
      },
      "outputs": [],
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(features, target1)\n",
        "x_train.shape, y_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Za7mQkd-nrKB",
      "metadata": {
        "id": "Za7mQkd-nrKB"
      },
      "source": [
        "#### Implementation using Normal Equation\n",
        "\n",
        "$\\theta = (X^T X)^{-1} . (X^T Y)$\n",
        "\n",
        "$θ$ is the hypothesis parameter that defines the coefficients\n",
        "\n",
        "$X$ is the input feature value of each instance\n",
        "\n",
        "$Y$ is Output value of each instance\n",
        "\n",
        "To solve the normal equation compute least-squares solution by using `scipy.linalg`\n",
        "\n",
        "Hint: [scipy.linalg.lstsq](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.lstsq.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66vjMjeHUTGO",
      "metadata": {
        "id": "66vjMjeHUTGO"
      },
      "outputs": [],
      "source": [
        "y = y_train.values\n",
        "# Adding ones to X\n",
        "X = np.append(np.ones((x_train.shape[0],1)),x_train, axis=1)\n",
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CYHpWmdFZ8JX",
      "metadata": {
        "id": "CYHpWmdFZ8JX"
      },
      "outputs": [],
      "source": [
        "# X_transpose * X\n",
        "X_t = np.transpose(X)\n",
        "X_Xt_dot = X_t.dot(X)\n",
        "\n",
        "# inverse of (X * X_transpose)\n",
        "temp1 = np.linalg.inv(X_Xt_dot)\n",
        "temp1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LZsJsIa0awLw",
      "metadata": {
        "id": "LZsJsIa0awLw"
      },
      "outputs": [],
      "source": [
        "# X_transpose * Y\n",
        "temp2 = X_t.dot(y)\n",
        "# Inverse of (X_transpose * X) * (X_transpose * Y)\n",
        "coefs = temp1.dot(temp2)\n",
        "coefs.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RxguajLI9G18",
      "metadata": {
        "id": "RxguajLI9G18"
      },
      "outputs": [],
      "source": [
        "# Above steps in one line\n",
        "y = y_train.values\n",
        "X_b = np.concatenate((np.ones((x_train.shape[0], 1)), x_train),axis=1)\n",
        "theta_star = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
        "theta_star.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2TmcGcR6Ff8D",
      "metadata": {
        "id": "2TmcGcR6Ff8D"
      },
      "outputs": [],
      "source": [
        "ypredict = X_b.dot(theta_star.ravel())\n",
        "mean_squared_error(ypredict, y_train.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Qa76xOU7lAhf",
      "metadata": {
        "id": "Qa76xOU7lAhf"
      },
      "outputs": [],
      "source": [
        "def Calc_MSE(X, y_test, coefficients):\n",
        "\n",
        "    X = np.append(np.ones((X.shape[0],1)),X, axis=1)\n",
        "    score = mean_squared_error(y_test.values, X.dot(coefficients))\n",
        "    return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2UGIpbF9cdFu",
      "metadata": {
        "id": "2UGIpbF9cdFu"
      },
      "outputs": [],
      "source": [
        "# objective(X,y, coefs)\n",
        "train_error = Calc_MSE(x_train, y_train, coefs)\n",
        "test_error = Calc_MSE(x_test, y_test, coefs)\n",
        "train_error, test_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pt-NMawJy69N",
      "metadata": {
        "id": "pt-NMawJy69N"
      },
      "outputs": [],
      "source": [
        "from scipy.linalg import lstsq\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(features, target1)\n",
        "y = y_train.values\n",
        "X = np.append(np.ones((x_train.shape[0],1)),x_train, axis=1)\n",
        "p, res, rnk, s = lstsq(X, y)\n",
        "print(np.sum((X.dot(p)-y)**2)/X.shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JfHHTPUl6joI",
      "metadata": {
        "id": "JfHHTPUl6joI"
      },
      "source": [
        "#### Implementing Linear regression using batch gradient descent\n",
        "\n",
        "Initialize the random coefficients and optimize the coefficients in the iterative process by calculating cost and finding the gradient.\n",
        "\n",
        "Hint: [link](https://medium.com/@lope.ai/multivariate-linear-regression-from-scratch-in-python-5c4f219be6a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6yCnKt_X6ixj",
      "metadata": {
        "id": "6yCnKt_X6ixj"
      },
      "outputs": [],
      "source": [
        "X = x_train\n",
        "y = y_train\n",
        "# Adding ones to X\n",
        "X = np.append(np.ones((X.shape[0],1)),X, axis=1)\n",
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CTP9zAAV4Gnd",
      "metadata": {
        "id": "CTP9zAAV4Gnd"
      },
      "outputs": [],
      "source": [
        "def cost_function(X, Y, B):\n",
        "  return mean_squared_error(Y, X.dot(B))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "m_w5i3sU4G00",
      "metadata": {
        "id": "m_w5i3sU4G00"
      },
      "outputs": [],
      "source": [
        "def batch_gradient_descent(X, Y, B, alpha, iterations):\n",
        "  cost_history = [0] * iterations\n",
        "  m = len(Y)\n",
        "  for iteration in range(iterations):\n",
        "    #print(iteration)\n",
        "    h = X.dot(B)\n",
        "    loss = h - Y #change the variable name\n",
        "    gradient = X.T.dot(loss) / m\n",
        "    B = B - alpha * gradient\n",
        "    cost = cost_function(X, Y, B)\n",
        "    cost_history[iteration] = cost\n",
        "  return B, cost_history\n",
        "\n",
        "B = np.random.randn(X.shape[1])\n",
        "alpha = 0.005\n",
        "iter_ = 50000\n",
        "newB, cost_history = batch_gradient_descent(X, y.values.ravel(), B, alpha, iter_)\n",
        "newB, cost_history[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZW3FyVdY9FSl",
      "metadata": {
        "id": "ZW3FyVdY9FSl"
      },
      "outputs": [],
      "source": [
        "# test error\n",
        "X_test = np.append(np.ones((x_test.shape[0],1)),x_test, axis=1)\n",
        "cost_function(X_test,y_test, newB)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PmZJdg33NlvA",
      "metadata": {
        "id": "PmZJdg33NlvA"
      },
      "source": [
        "#### SGD Regressor\n",
        "\n",
        "Use the SGD regressor from sklearn with one target variable and find the error\n",
        "\n",
        "Hint: [SGDRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fU1-w4XRNlLA",
      "metadata": {
        "id": "fU1-w4XRNlLA"
      },
      "outputs": [],
      "source": [
        "sgd = linear_model.SGDRegressor()\n",
        "sgd = sgd.fit(x_train, y_train)\n",
        "print(\"score is \",sgd.score(x_test, y_test))\n",
        "mean_squared_error(sgd.predict(x_test), y_test )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "legal-engagement",
      "metadata": {
        "id": "legal-engagement"
      },
      "source": [
        "### Linear regression using sklearn (2 points)\n",
        "\n",
        "Implement the linear regression model using sklearn with two variables in target (casual, registered)\n",
        "\n",
        "Hint: [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "thirty-effort",
      "metadata": {
        "id": "thirty-effort"
      },
      "source": [
        "#### Select the features  and split the data into train and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "complicated-reserve",
      "metadata": {
        "id": "complicated-reserve"
      },
      "outputs": [],
      "source": [
        "target2 = bikeshare1['cnt']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "finnish-reviewer",
      "metadata": {
        "id": "finnish-reviewer"
      },
      "outputs": [],
      "source": [
        "xtrain, xtest, ytrain, ytest = train_test_split(features, target1)\n",
        "xtrain.shape, ytrain.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "derived-saturday",
      "metadata": {
        "id": "derived-saturday"
      },
      "outputs": [],
      "source": [
        "regr_linear = linear_model.LinearRegression()\n",
        "regr_linear.fit(xtrain, ytrain)\n",
        "predicted = regr_linear.predict(xtest)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "spiritual-dictionary",
      "metadata": {
        "id": "spiritual-dictionary"
      },
      "source": [
        "#### Calculate the mean squared error of the actual and predicted data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dimensional-probability",
      "metadata": {
        "id": "dimensional-probability"
      },
      "outputs": [],
      "source": [
        "mse_linear = mean_squared_error(ytest, predicted, multioutput = 'uniform_average')\n",
        "mse_linear"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "quality-morgan",
      "metadata": {
        "id": "quality-morgan"
      },
      "source": [
        "#### Calculate the $R^2$ (coefficient of determination) of the actual and predicted data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "important-jacket",
      "metadata": {
        "id": "important-jacket"
      },
      "outputs": [],
      "source": [
        "r2_score(ytrain, regr_linear.predict(xtrain)), r2_score(ytest, predicted)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "infinite-victim",
      "metadata": {
        "id": "infinite-victim"
      },
      "source": [
        "#### summarize the importance of features and create a bar chart\n",
        "\n",
        "Prediction is the weighted sum of the input values e.g. linear regression, and extensions that add regularization, such as ridge regression and the elastic net find a set of coefficients to use in the weighted sum to make a prediction. These coefficients can be used directly as a crude type of feature importance score.\n",
        "\n",
        "This assumes that the input variables have the same scale or have been scaled prior to fitting a model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "affected-walker",
      "metadata": {
        "id": "affected-walker"
      },
      "outputs": [],
      "source": [
        "# coefficients of casual variable\n",
        "importance_casual = regr_linear.coef_[0,]\n",
        "plt.bar([x for x in range(len(importance_casual))], importance_casual)\n",
        "plt.xticks(range(59))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "informal-bobby",
      "metadata": {
        "id": "informal-bobby"
      },
      "outputs": [],
      "source": [
        "# coefficients of registered variable\n",
        "# importance_registered = regr_linear.coef_[1,]\n",
        "# plt.bar([x for x in range(len(importance_registered))], importance_registered)\n",
        "# plt.xticks(range(59))\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "convinced-snowboard",
      "metadata": {
        "id": "convinced-snowboard"
      },
      "source": [
        "### Regularization methods (2 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "twenty-italic",
      "metadata": {
        "id": "twenty-italic"
      },
      "source": [
        "#### Apply lasso regression\n",
        "\n",
        "* Apply Lasso regression with different alpha values given below and find the best alpha that gives the least error.\n",
        "* Calculate the metrics for the actual and predicted\n",
        "\n",
        "Hint: [Lasso](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "psychological-blake",
      "metadata": {
        "id": "psychological-blake"
      },
      "outputs": [],
      "source": [
        "# setting up alpha\n",
        "alpha = [0.0001, 0.001,0.01, 0.1, 1, 10, 100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "existing-sigma",
      "metadata": {
        "id": "existing-sigma"
      },
      "outputs": [],
      "source": [
        "for a in alpha:\n",
        "    regr_lasso = linear_model.Lasso(alpha = a)\n",
        "    regr_lasso.fit(xtrain, ytrain)\n",
        "    mse_lasso_sk = mean_squared_error(ytest, regr_lasso.predict(xtest), multioutput = 'uniform_average')\n",
        "    print(a, \"=====\",mse_lasso_sk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "different-helicopter",
      "metadata": {
        "id": "different-helicopter"
      },
      "outputs": [],
      "source": [
        "# with best alpha chosen from above\n",
        "regr_lasso = linear_model.Lasso(alpha = 0.0001)\n",
        "regr_lasso.fit(xtrain, ytrain)\n",
        "mse_lasso_sk = mean_squared_error(ytest, regr_lasso.predict(xtest), multioutput = 'uniform_average')\n",
        "print(\"Lasso MSE:\",mse_lasso_sk)\n",
        "print(\"Lasso r2_score\",r2_score(ytrain, regr_lasso.predict(xtrain)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "figured-effectiveness",
      "metadata": {
        "id": "figured-effectiveness"
      },
      "source": [
        "#### Apply ridge regression\n",
        "\n",
        "* Apply Lasso regression with different alpha values given and find the best alpha that gives the least error.\n",
        "* Calculate the metrics for the actual and predicted\n",
        "\n",
        "Hint: [Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "curious-initial",
      "metadata": {
        "id": "curious-initial"
      },
      "outputs": [],
      "source": [
        "for a in alpha:\n",
        "    regr_ridge = linear_model.Ridge(alpha = a)\n",
        "    regr_ridge.fit(xtrain, ytrain)\n",
        "    mse_ridge_sk = mean_squared_error(ytest, regr_ridge.predict(xtest), multioutput = 'uniform_average')\n",
        "    print(a, \"=====\",mse_ridge_sk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "angry-miniature",
      "metadata": {
        "id": "angry-miniature"
      },
      "outputs": [],
      "source": [
        "# with best alpha chosen from above\n",
        "regr_ridge = linear_model.Ridge(alpha = 0.001)\n",
        "regr_ridge.fit(xtrain, ytrain)\n",
        "mse_ridge_sk = mean_squared_error(ytest, regr_ridge.predict(xtest), multioutput = 'uniform_average')\n",
        "print(\"Ridge MSE:\",mse_ridge_sk)\n",
        "print(\"Ridge r2_score:\",r2_score(ytrain, regr_ridge.predict(xtrain)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "exposed-bottom",
      "metadata": {
        "id": "exposed-bottom"
      },
      "source": [
        "#### Apply elasticnet regression\n",
        "\n",
        "* Apply elasticnet regression with different alpha values given and find the best alpha that gives the least error.\n",
        "* Calculate the metrics for the actual and predicted\n",
        "\n",
        "Hint: [ElasticNet](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "shared-belief",
      "metadata": {
        "id": "shared-belief"
      },
      "outputs": [],
      "source": [
        "for a in alpha:\n",
        "    elasticnet_regr = linear_model.ElasticNet(alpha=a)\n",
        "    elasticnet_regr.fit(xtrain, ytrain)\n",
        "    mse_elatic_sk = mean_squared_error(ytest, elasticnet_regr.predict(xtest), multioutput = 'uniform_average')\n",
        "    print(a,\"====\",mse_elatic_sk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "weighted-fossil",
      "metadata": {
        "id": "weighted-fossil"
      },
      "outputs": [],
      "source": [
        "# Elasticnet\n",
        "elasticnet_regr = linear_model.ElasticNet(alpha=0.01)\n",
        "elasticnet_regr.fit(xtrain, ytrain)\n",
        "mse_elatic_sk = mean_squared_error(ytest, elasticnet_regr.predict(xtest), multioutput = 'uniform_average')\n",
        "print(\"Elasticnet MSE:\",mse_elatic_sk)\n",
        "print(\"Elasticnet r2_score\",r2_score(ytrain, elasticnet_regr.predict(xtrain)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "i5l5ZviEMy-o",
      "metadata": {
        "id": "i5l5ZviEMy-o"
      },
      "source": [
        "* **Use the two variables (`Casual, Registered`) in target and find the error by implementing Linear Regression model from sklearn**\n",
        "* Describe your interpretation of the methods that are used to implement linear regression covered in this mini project.\n",
        "* Comment on performance of the algorithms/methods used.\n",
        "* Comment about the nature of the data and fitment of linear regression for this data.\n",
        "* Can you perform a non linear curve fitting using linear regression? If yes, How?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1sGHEF7MovkU",
      "metadata": {
        "id": "1sGHEF7MovkU"
      },
      "source": [
        "#### Solution of Linear regression with 2 variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AgcAUSDxos_D",
      "metadata": {
        "id": "AgcAUSDxos_D"
      },
      "outputs": [],
      "source": [
        "target2 = bikeshare1[['casual','registered']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dz9x-JE5os_b",
      "metadata": {
        "id": "dz9x-JE5os_b"
      },
      "outputs": [],
      "source": [
        "xtrain, xtest, ytrain, ytest = train_test_split(features, target1)\n",
        "xtrain.shape, ytrain.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YHY9Rb4Fos_g",
      "metadata": {
        "id": "YHY9Rb4Fos_g"
      },
      "outputs": [],
      "source": [
        "regr_linear = linear_model.LinearRegression()\n",
        "regr_linear.fit(xtrain, ytrain)\n",
        "predicted = regr_linear.predict(xtest)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Xt4K5oBTos_i",
      "metadata": {
        "id": "Xt4K5oBTos_i"
      },
      "outputs": [],
      "source": [
        "mse_linear = mean_squared_error(ytest, predicted, multioutput = 'uniform_average')\n",
        "mse_linear"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4w8NwzOYos_j",
      "metadata": {
        "id": "4w8NwzOYos_j"
      },
      "outputs": [],
      "source": [
        "r2_score(ytrain, regr_linear.predict(xtrain)), r2_score(ytest, predicted)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
