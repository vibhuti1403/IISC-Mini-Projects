{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "intensive-feature",
      "metadata": {
        "id": "intensive-feature"
      },
      "source": [
        "# Advanced Certification Program in Computational Data Science\n",
        "## A program by IISc and TalentSprint\n",
        "### Mini-Project: End-to-end analytics application using Pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "buried-qualification",
      "metadata": {
        "id": "buried-qualification"
      },
      "source": [
        "## Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "smaller-diana",
      "metadata": {
        "id": "smaller-diana"
      },
      "source": [
        "Perform sentiment classification by analyzing the tweets data with Pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "opposite-defense",
      "metadata": {
        "id": "opposite-defense"
      },
      "source": [
        "## Learning Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "incoming-professor",
      "metadata": {
        "id": "incoming-professor"
      },
      "source": [
        "At the end of the mini-project, you will be able to :\n",
        "\n",
        "* analyze the text data using pyspark\n",
        "* derive the insights and visualize the data\n",
        "* implement feature extraction and classify the data\n",
        "* train the classification model and deploy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "varied-emission",
      "metadata": {
        "id": "varied-emission"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "usual-suffering",
      "metadata": {
        "id": "usual-suffering"
      },
      "source": [
        "The dataset chosen for this mini-project is **[Twitter US Airline Sentiment](https://data.world/socialmediadata/twitter-us-airline-sentiment)**. It is a record of tweets about airlines in the US. It was created by scraping Twitter data from February 2015. Contributors were asked to first classify positive, negative, and neutral tweets, followed by categorizing negative reasons (such as \"late flight\" or \"rude service\").  Along with other information, it contains ID of a Tweet, the sentiment of a tweet ( neutral, negative and positive), reason for a negative tweet, name of airline and text of a tweet."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "original-tsunami",
      "metadata": {
        "id": "original-tsunami"
      },
      "source": [
        "## Information"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "premier-northeast",
      "metadata": {
        "id": "premier-northeast"
      },
      "source": [
        "The airline industry is a very competitive market that has grown rapidly in the past 2 decades. Airline companies resort to traditional customer feedback forms which in turn are very tedious and time consuming. This is where Twitter data serves as a good source to gather customer feedback tweets and perform sentiment analysis. This dataset comprises of tweets for 6 major US Airlines and a multi-class classification can be performed to categorize the sentiment (neutral, negative, positive). For this mini-project we will start with pre-processing techniques to clean the tweets and then represent these tweets as vectors. A classification algorithm will be used to predict the sentiment for unseen tweets data. The end-to-end analytics will be performed using Pyspark."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BewwTjZaJojg",
      "metadata": {
        "id": "BewwTjZaJojg"
      },
      "source": [
        "## Grading = 10 Points"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "younger-macro",
      "metadata": {
        "id": "younger-macro"
      },
      "source": [
        "#### Install Pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Hxl6vbeA-whf",
      "metadata": {
        "cellView": "form",
        "id": "Hxl6vbeA-whf"
      },
      "outputs": [],
      "source": [
        "#@title Install packages and download the dataset\n",
        "!pip -qq install pyspark\n",
        "!pip -qq install handyspark\n",
        "!wget -qq https://cdn.iisc.talentsprint.com/CDS/MiniProjects/US_Airline_Tweets.csv\n",
        "print(\"Packages installed successfully and dataset downloaded!!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rough-battlefield",
      "metadata": {
        "id": "rough-battlefield"
      },
      "source": [
        "#### Import required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cheap-workplace",
      "metadata": {
        "id": "cheap-workplace"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from handyspark import *\n",
        "#import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "import re\n",
        "import string\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.feature import CountVectorizer\n",
        "from pyspark.ml.classification import NaiveBayes\n",
        "from pyspark.sql.types import ArrayType, StringType"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "useful-meter",
      "metadata": {
        "id": "useful-meter"
      },
      "outputs": [],
      "source": [
        "# NLTK imports\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "# Download stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pregnant-april",
      "metadata": {
        "id": "pregnant-april"
      },
      "source": [
        "### Data Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "toxic-baseball",
      "metadata": {
        "id": "toxic-baseball"
      },
      "source": [
        "#### Start a Spark Session\n",
        "\n",
        "Spark session is a combined entry point of a Spark application, which came into implementation from Spark 2.0. It provides a way to interact with various Spark functionalities, with a lesser number of constructs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "christian-rental",
      "metadata": {
        "id": "christian-rental"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder.appName('TwitterSentiment').getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "casual-narrative",
      "metadata": {
        "id": "casual-narrative"
      },
      "source": [
        "#### Load the data and infer the schema\n",
        "\n",
        "To load the dataset use the `read.csv` with `inferSchema` and `header` as parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "natural-lexington",
      "metadata": {
        "id": "natural-lexington"
      },
      "outputs": [],
      "source": [
        "dataset = spark.read.csv(\"/content/US_Airline_Tweets.csv\",inferSchema=True,header=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "alleged-married",
      "metadata": {
        "id": "alleged-married"
      },
      "outputs": [],
      "source": [
        "dataset.show(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dulEo4uqrw-6",
      "metadata": {
        "id": "dulEo4uqrw-6"
      },
      "outputs": [],
      "source": [
        "dataset.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "human-samoa",
      "metadata": {
        "id": "human-samoa"
      },
      "source": [
        "### EDA & Visualization ( 2 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "angry-canyon",
      "metadata": {
        "id": "angry-canyon"
      },
      "source": [
        "#### Visualize the horizontal barplot of airline_sentiment (positive, negative, neutral)\n",
        "\n",
        "Convert the data to handyspark and remove the other records from the column except 3 values mentioned above and plot the graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "accomplished-crest",
      "metadata": {
        "id": "accomplished-crest"
      },
      "outputs": [],
      "source": [
        "handyDf = dataset.toHandy()\n",
        "\n",
        "handyDf = handyDf.filter(handyDf[\"airline_sentiment\"].isin([\"neutral\",\"positive\",\"negative\"]))\n",
        "sentiments = handyDf.cols['airline_sentiment'][:].value_counts() #.isin(['neutral','positive','negative'])]\n",
        "sentiments.plot.barh()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YEQ-8w9bsAR2",
      "metadata": {
        "id": "YEQ-8w9bsAR2"
      },
      "outputs": [],
      "source": [
        "handyDf.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "serial-cedar",
      "metadata": {
        "id": "serial-cedar"
      },
      "source": [
        "#### Plot the number of tweets received for each airline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "peripheral-bookmark",
      "metadata": {
        "id": "peripheral-bookmark"
      },
      "outputs": [],
      "source": [
        "handyDf.cols['airline'][:].value_counts().plot.bar()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jGwC-rM2-plb",
      "metadata": {
        "id": "jGwC-rM2-plb"
      },
      "source": [
        "#### Visualize a stacked barchart of 6 US airlines and 3 sentiments on each bar\n",
        "\n",
        "* Display the count corresponding to each sentiment in each bar. [hint](https://priteshbgohil.medium.com/stacked-bar-chart-in-python-ddc0781f7d5f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "focal-david",
      "metadata": {
        "id": "focal-david"
      },
      "outputs": [],
      "source": [
        "grouped = handyDf.groupby('airline','airline_sentiment').agg(count('airline_sentiment'))\n",
        "neutral = grouped.filter(grouped['airline_sentiment']=='neutral').cols['count(airline_sentiment)'][:]\n",
        "positive = grouped.filter(grouped['airline_sentiment']=='positive').cols['count(airline_sentiment)'][:]\n",
        "negative = grouped.filter(grouped['airline_sentiment']=='negative').cols['count(airline_sentiment)'][:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "convinced-chance",
      "metadata": {
        "id": "convinced-chance"
      },
      "outputs": [],
      "source": [
        "airlines = list(set(handyDf.cols['airline'][:]))\n",
        "plt.figure(figsize=(10,7))\n",
        "\n",
        "ax1 = plt.bar(airlines,neutral.values,color='y',label='neutral')\n",
        "ax2 = plt.bar(airlines,positive.values,bottom=neutral.values,color='b',label='positive')\n",
        "ax3 = plt.bar(airlines,negative.values,bottom=neutral.values+positive.values,color='r',label='negative')\n",
        "for r1, r2, r3 in zip(ax1, ax2, ax3):\n",
        "    h1 = r1.get_height()\n",
        "    h2 = r2.get_height()\n",
        "    h3 = r3.get_height()\n",
        "    plt.text(r1.get_x() + r1.get_width() / 2., h1 / 2., \"%d\" % h1, ha=\"center\", va=\"center\", color=\"white\", fontsize=16, fontweight=\"bold\")\n",
        "    plt.text(r2.get_x() + r2.get_width() / 2., h1 + h2 / 2., \"%d\" % h2, ha=\"center\", va=\"center\", color=\"white\", fontsize=16, fontweight=\"bold\")\n",
        "    plt.text(r3.get_x() + r3.get_width() / 2., h1 + h2 + h3 / 2., \"%d\" % h3, ha=\"center\", va=\"center\", color=\"white\", fontsize=16, fontweight=\"bold\")\n",
        "plt.legend()#['neutral','positive','negative'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ahead-control",
      "metadata": {
        "id": "ahead-control"
      },
      "source": [
        "#### Visualize the horizontal barplot of negative reasons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "coordinate-rough",
      "metadata": {
        "id": "coordinate-rough"
      },
      "outputs": [],
      "source": [
        "handyDf1 = handyDf.filter(~handyDf[\"negativereason\"].isNull())\n",
        "negativereason = handyDf1.cols['negativereason'][:].value_counts() #.isin(['neutral','positive','negative'])]\n",
        "negativereason.plot.barh()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "scheduled-logistics",
      "metadata": {
        "id": "scheduled-logistics"
      },
      "source": [
        "### Pre-processing (3 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pharmaceutical-agency",
      "metadata": {
        "id": "pharmaceutical-agency"
      },
      "source": [
        "#### Check the null values and drop the records where the text value is null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "deadly-abraham",
      "metadata": {
        "id": "deadly-abraham"
      },
      "outputs": [],
      "source": [
        "# check the null values in text column\n",
        "dataset.filter(dataset.text.isNull()).count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "molecular-sword",
      "metadata": {
        "id": "molecular-sword"
      },
      "outputs": [],
      "source": [
        "# filter out and remove null values from text column\n",
        "dataset_filtered = dataset.filter(dataset.text.isNotNull())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "auburn-aggregate",
      "metadata": {
        "id": "auburn-aggregate"
      },
      "outputs": [],
      "source": [
        "# verify the null count\n",
        "dataset.count(), dataset_filtered.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "convinced-batch",
      "metadata": {
        "id": "convinced-batch"
      },
      "source": [
        "#### Fill the null values with 0 in all the columns except the target\n",
        "\n",
        "The target should not be empty. Ensure that all features are integer type, convert if needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "thorough-jones",
      "metadata": {
        "id": "thorough-jones"
      },
      "outputs": [],
      "source": [
        "fillNull = udf(lambda x: 0 if x == None else x)\n",
        "dataset_filtered = dataset_filtered.withColumn('negativereason_confidence',fillNull(dataset_filtered['negativereason_confidence']).astype('int'))\n",
        "dataset_filtered = dataset_filtered.withColumn('airline_sentiment_confidence',fillNull(dataset_filtered['airline_sentiment_confidence']).astype('int'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8NYdYNat-ipT",
      "metadata": {
        "id": "8NYdYNat-ipT"
      },
      "source": [
        "#### Preprocessing and cleaning the tweets\n",
        "\n",
        "* Convert the text to lower case\n",
        "* Remove usernames, hashtags and links from the text (tweets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "studied-blowing",
      "metadata": {
        "id": "studied-blowing"
      },
      "outputs": [],
      "source": [
        "puncts = \"!#$%&\\()*+,-./:;<=>?@[\\\\]^_`{|}~\"\n",
        "def words_process(text):\n",
        "    text = text.lower() # lowercase\n",
        "    text.replace(r'http?://[^\\s<>\"]+|www\\.[^\\s<>\"]+', '') # Removing hyperlinks from all the tweets\n",
        "    text.replace('\\d+', '') # Removing numbers from all the tweets\n",
        "    text = \" \".join([i for i in text.split() if i.find(\"@\")== -1]) # removing usernames\n",
        "    text = text.replace('#','') # Removing hashtags, including the text, from all the tweets\n",
        "    text = re.sub(r'[^a-zA-Z0-9 ]',r'',text)\n",
        "    return text\n",
        "\n",
        "words = udf(words_process)\n",
        "dataset_filtered = dataset_filtered.withColumn('text_processed',words(dataset_filtered['text']))\n",
        "dataset_filtered.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "subtle-spain",
      "metadata": {
        "id": "subtle-spain"
      },
      "source": [
        "#### Tokenize the text sentence into words using nltk sentence tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yellow-wholesale",
      "metadata": {
        "id": "yellow-wholesale"
      },
      "outputs": [],
      "source": [
        "word_udf = udf(lambda x: word_tokenize(x), ArrayType(StringType()))\n",
        "dataset_filtered = dataset_filtered.withColumn(\"wordss\", word_udf(\"text_processed\"))\n",
        "dataset_filtered.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "recognized-wrist",
      "metadata": {
        "id": "recognized-wrist"
      },
      "source": [
        "#### Remove the stopwords from tokenized words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "scientific-guinea",
      "metadata": {
        "id": "scientific-guinea"
      },
      "outputs": [],
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "print(stop_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pleased-skiing",
      "metadata": {
        "id": "pleased-skiing"
      },
      "outputs": [],
      "source": [
        "punct_udf1 = udf(lambda x: [w for w in x if not w in stop_words])\n",
        "dataset_filtered = dataset_filtered.withColumn(\"wordss\", punct_udf1(\"wordss\"))\n",
        "\n",
        "array_udf = udf(lambda x: x, ArrayType(StringType()))\n",
        "dataset_filtered = dataset_filtered.withColumn(\"wordss\", array_udf(\"wordss\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "persistent-projection",
      "metadata": {
        "id": "persistent-projection"
      },
      "source": [
        "#### Apply Lemmatization to the words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "asian-parade",
      "metadata": {
        "id": "asian-parade"
      },
      "outputs": [],
      "source": [
        "def lemmatize(text_arr):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    return [lemmatizer.lemmatize(word) for word in text_arr]\n",
        "lem = udf(lemmatize)\n",
        "dataset_filtered = dataset_filtered.withColumn(\"wordss\", lem(\"wordss\"))\n",
        "\n",
        "array_udf = udf(lambda x: x, ArrayType(StringType()))\n",
        "dataset_filtered = dataset_filtered.withColumn(\"wordss\", array_udf(\"wordss\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "successful-telescope",
      "metadata": {
        "id": "successful-telescope"
      },
      "outputs": [],
      "source": [
        "dataset_filtered.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UR5FnuSn-c-S",
      "metadata": {
        "id": "UR5FnuSn-c-S"
      },
      "source": [
        "### Feature Extraction (3 points)\n",
        "\n",
        "Create the useful features from the text column to train the model\n",
        "\n",
        "For example:\n",
        "* Length of the tweet\n",
        "* No. of hashtags in the tweet starting with '#'\n",
        "* No. of mentions in the tweet starting with '@'\n",
        "\n",
        "Hint: create a new column for each of the above features"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "operating-phenomenon",
      "metadata": {
        "id": "operating-phenomenon"
      },
      "source": [
        "* create a column \"Length of tweet\" using `udf` function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adequate-yahoo",
      "metadata": {
        "id": "adequate-yahoo"
      },
      "outputs": [],
      "source": [
        "# CODE HERE\n",
        "length = udf(lambda x: len(x))\n",
        "\n",
        "dataset_filtered = dataset_filtered.withColumn('tweet_length', length(dataset_filtered['text_processed']).astype('int'))\n",
        "dataset_filtered.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "adjustable-knowing",
      "metadata": {
        "id": "adjustable-knowing"
      },
      "source": [
        "* Create a new column \"No.of Hashtags\" in each tweet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "unauthorized-venue",
      "metadata": {
        "id": "unauthorized-venue"
      },
      "outputs": [],
      "source": [
        "num_hashtags = udf(lambda x : len(re.compile(r\"#(\\w+)\").findall(x)))\n",
        "\n",
        "dataset_filtered = dataset_filtered.withColumn('num_hashtags', num_hashtags(dataset_filtered['text']).astype('int'))\n",
        "dataset_filtered.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "south-upper",
      "metadata": {
        "id": "south-upper"
      },
      "source": [
        "* Create a new column \"No.of Mentions\" in each tweet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sticky-budapest",
      "metadata": {
        "id": "sticky-budapest"
      },
      "outputs": [],
      "source": [
        "num_mentions = udf(lambda x : len(re.compile(r\"@(\\w+)\").findall(x)))\n",
        "\n",
        "dataset_filtered = dataset_filtered.withColumn('num_mentions', num_mentions(dataset_filtered['text']).astype('int'))\n",
        "dataset_filtered.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "atlantic-installation",
      "metadata": {
        "id": "atlantic-installation"
      },
      "source": [
        "* Create a new column \"Punctuation Count\" in each tweet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "median-norfolk",
      "metadata": {
        "id": "median-norfolk"
      },
      "outputs": [],
      "source": [
        "def punctCount1(text):\n",
        "    sum = 0\n",
        "    for i in text:\n",
        "        if i in \"!$%&'()*+,-./:;<=>?[\\]^_`{|}~\":\n",
        "            sum +=1\n",
        "    return sum\n",
        "\n",
        "punctCount = udf(punctCount1)\n",
        "dataset_filtered = dataset_filtered.withColumn('PunctCount',punctCount(dataset_filtered['text']).astype('int'))\n",
        "dataset_filtered.select('PunctCount').show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bulgarian-packaging",
      "metadata": {
        "id": "bulgarian-packaging"
      },
      "source": [
        "* Create a new column \"Type of Punctuations\" used in each tweet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lyric-processor",
      "metadata": {
        "id": "lyric-processor"
      },
      "outputs": [],
      "source": [
        "def types_punctuation(text):\n",
        "    return len(set([i for i in text if i in \"!$%&'()*+,-./:;<=>?[\\]^_`{|}~\"]))\n",
        "\n",
        "typePunct = udf(types_punctuation)\n",
        "dataset_filtered = dataset_filtered.withColumn('typePunct',typePunct(dataset_filtered['text']).astype('int'))\n",
        "dataset_filtered.show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oUhFSWtT-XIO",
      "metadata": {
        "id": "oUhFSWtT-XIO"
      },
      "source": [
        "#### Get the features by applying CountVectorizer\n",
        "CountVectorizer converts the list of tokens to vectors of token counts. See the [documentation](https://spark.apache.org/docs/latest/ml-features.html#countvectorizer) for details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "christian-voltage",
      "metadata": {
        "id": "christian-voltage"
      },
      "outputs": [],
      "source": [
        "count = CountVectorizer(inputCol=\"wordss\", outputCol=\"rawFeatures\")\n",
        "count_model = count.fit(dataset_filtered)\n",
        "featurizedData = count_model.transform(dataset_filtered)\n",
        "featurizedData.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6xH0J7Bf-SmB",
      "metadata": {
        "id": "6xH0J7Bf-SmB"
      },
      "source": [
        "#### Encode the labels\n",
        "\n",
        "Using the `udf` function encode the string values of *airline_sentiment* to integers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "assigned-trout",
      "metadata": {
        "id": "assigned-trout"
      },
      "outputs": [],
      "source": [
        "def LabelEncoder(x):\n",
        "    if x == 'positive':\n",
        "        return 0\n",
        "    elif x == 'negative':\n",
        "        return 1\n",
        "    return 2\n",
        "encoder = udf(LabelEncoder)\n",
        "featurizedData = featurizedData.withColumn('label', encoder(featurizedData['airline_sentiment']).astype('int'))\n",
        "featurizedData.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vital-sight",
      "metadata": {
        "id": "vital-sight"
      },
      "source": [
        "### Train the classifier the evaluate (1 point)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "attempted-lender",
      "metadata": {
        "id": "attempted-lender"
      },
      "source": [
        "Create vector assembler with the selected features to train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "stuffed-seating",
      "metadata": {
        "id": "stuffed-seating"
      },
      "outputs": [],
      "source": [
        "featureassembler = VectorAssembler(inputCols=['rawFeatures','airline_sentiment_confidence','negativereason_confidence',\n",
        "                                              'tweet_length','num_hashtags','num_mentions','retweet_count','PunctCount','typePunct']\n",
        "                                   ,outputCol='features')\n",
        "features = featureassembler.transform(featurizedData)\n",
        "features.select('features').show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bored-metropolitan",
      "metadata": {
        "id": "bored-metropolitan"
      },
      "source": [
        "#### Arrange features and label and split them into train and test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "brazilian-ranking",
      "metadata": {
        "id": "brazilian-ranking"
      },
      "outputs": [],
      "source": [
        "featuresAndLabels = features.withColumn('labels',featurizedData['label'])\n",
        "final = featuresAndLabels.select('features','labels')\n",
        "final.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "structured-actress",
      "metadata": {
        "id": "structured-actress"
      },
      "outputs": [],
      "source": [
        "train_data,test_data = final.randomSplit([0.75,0.25])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VjmU9f8o-Nme",
      "metadata": {
        "id": "VjmU9f8o-Nme"
      },
      "source": [
        "#### Train the model with train data and make predictions on the test data\n",
        "\n",
        "For classification of text data, implement NaiveBayes classifier. It is a probabilistic machine learning model.\n",
        "\n",
        "For more information about **NaiveBayes Classifier**, click [here](https://spark.apache.org/docs/latest/ml-classification-regression.html#naive-bayes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "destroyed-religion",
      "metadata": {
        "id": "destroyed-religion"
      },
      "outputs": [],
      "source": [
        "nb = NaiveBayes(featuresCol='features', labelCol='labels')\n",
        "model = nb.fit(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "seeing-money",
      "metadata": {
        "id": "seeing-money"
      },
      "outputs": [],
      "source": [
        "# get the predictions\n",
        "pred_results = model.transform(test_data)\n",
        "pred_results.select('prediction').show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "flQXk07t-Kuk",
      "metadata": {
        "id": "flQXk07t-Kuk"
      },
      "source": [
        "#### Evaluate the model and find the accuracy\n",
        "\n",
        "Compare the labels and predictions and find how many are correct.\n",
        "\n",
        "To find the accuracy, get the count of correct predictions from test data and divide by the total amount of test dataset.\n",
        "\n",
        "**Hint:** convert the predictions dataframe to pandas and compare with labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "greenhouse-crack",
      "metadata": {
        "id": "greenhouse-crack"
      },
      "outputs": [],
      "source": [
        "# converting to pandas df\n",
        "preds = pred_results.select('labels','prediction').toPandas()\n",
        "preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "grateful-iceland",
      "metadata": {
        "id": "grateful-iceland"
      },
      "outputs": [],
      "source": [
        "# comparing labels and predictions of test data\n",
        "(preds['labels'] == preds['prediction']).sum() / len(preds)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zNK7mTeS8mB4",
      "metadata": {
        "id": "zNK7mTeS8mB4"
      },
      "source": [
        "### Deployment (1 point)\n",
        "\n",
        "Let's integrate all the above code snippets in app.py and run it with **Streamlit**.\n",
        "\n",
        "From the start (data loading step), place every code in app.py including data preprocessing, feature extraction and model training.\n",
        "\n",
        "* implement the `predict_users_Input()` function which takes one tweet input from user and returns the prediction using the trained model.\n",
        "\n",
        "* use the same preprocessing techniques and features extraction used for train data on user input.\n",
        "\n",
        "* user input can be captured from the textbox from **Streamlit** app. Action is triggered when predict button is clicked and user input is classified using `predict_users_Input()` function.\n",
        "\n",
        "For More information about Streamlit, click [here](https://docs.streamlit.io/en/stable/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wnlD5cS9zcsL",
      "metadata": {
        "id": "wnlD5cS9zcsL"
      },
      "outputs": [],
      "source": [
        "!pip install -qq streamlit\n",
        "!pip install -qq colab-everything"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jmpuLMdD7ih9",
      "metadata": {
        "id": "jmpuLMdD7ih9"
      },
      "outputs": [],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "import re\n",
        "import string\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.feature import CountVectorizer\n",
        "from pyspark.ml.classification import NaiveBayes\n",
        "from pyspark.sql.types import ArrayType, StringType\n",
        "from handyspark import *\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "st.write(\"Creating a spark session :heavy_check_mark:\")\n",
        "spark = SparkSession.builder.appName('TwitterSentiment').getOrCreate()\n",
        "dataset = spark.read.csv(\"/content/US_Airline_Tweets.csv\",inferSchema=True,header=True)\n",
        "dataset_filtered = dataset.filter(dataset.text.isNotNull())\n",
        "fillNull = udf(lambda x: 0 if x == None else x)\n",
        "dataset_filtered = dataset_filtered.withColumn('negativereason_confidence',fillNull(dataset_filtered['negativereason_confidence']).astype('int'))\n",
        "dataset_filtered = dataset_filtered.withColumn('airline_sentiment_confidence',fillNull(dataset_filtered['airline_sentiment_confidence']).astype('int'))\n",
        "\n",
        "hdf = dataset.toHandy()\n",
        "\n",
        "def words_process(text):\n",
        "    text = text.lower() # lowercase\n",
        "    text.replace(r'http?://[^\\s<>\"]+|www\\.[^\\s<>\"]+', '') # Removing hyperlinks from all the tweets\n",
        "    text.replace('\\d+', '') # Removing numbers from all the tweets\n",
        "    text = \" \".join([i for i in text.split() if i.find(\"@\")== -1]) # removing usernames\n",
        "    text = text.replace('#','') # Removing hashtags, including the text, from all the tweets\n",
        "    return text\n",
        "\n",
        "words = udf(words_process)\n",
        "dataset_filtered = dataset_filtered.withColumn('text_processed',words(dataset_filtered['text']))\n",
        "\n",
        "word_udf = udf(lambda x: word_tokenize(x), ArrayType(StringType()))\n",
        "dataset_filtered = dataset_filtered.withColumn(\"wordss\", word_udf(\"text_processed\"))\n",
        "stop_words = set(stopwords.words('english'))\n",
        "punct_udf1 = udf(lambda x: [w for w in x if not w in stop_words])\n",
        "dataset_filtered = dataset_filtered.withColumn(\"wordss\", punct_udf1(\"wordss\"))\n",
        "\n",
        "array_udf = udf(lambda x: x, ArrayType(StringType()))\n",
        "dataset_filtered = dataset_filtered.withColumn(\"wordss\", array_udf(\"wordss\"))\n",
        "\n",
        "def lemmatize(text_arr):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    return [lemmatizer.lemmatize(word) for word in text_arr]\n",
        "lem = udf(lemmatize)\n",
        "dataset_filtered = dataset_filtered.withColumn(\"wordss\", lem(\"wordss\"))\n",
        "st.write(\"Preprocessing Done! :heavy_check_mark:\")\n",
        "\n",
        "array_udf = udf(lambda x: x, ArrayType(StringType()))\n",
        "dataset_filtered = dataset_filtered.withColumn(\"wordss\", array_udf(\"wordss\"))\n",
        "\n",
        "length = udf(lambda x: len(x))\n",
        "dataset_filtered = dataset_filtered.withColumn('tweet_length', length(dataset_filtered['text']).astype('int'))\n",
        "num_hashtags = udf(lambda x : len(re.compile(r\"#(\\w+)\").findall(x)) if len(re.compile(r\"#(\\w+)\").findall(x)) > 0 else 0)\n",
        "\n",
        "dataset_filtered = dataset_filtered.withColumn('num_hashtags', num_hashtags(dataset_filtered['text']).astype('int'))\n",
        "num_mentions = udf(lambda x : len(re.compile(r\"@(\\w+)\").findall(x)) if len(re.compile(r\"@(\\w+)\").findall(x)) > 0 else 0)\n",
        "\n",
        "dataset_filtered = dataset_filtered.withColumn('num_mentions', num_mentions(dataset_filtered['text']).astype('int'))\n",
        "def punctCount1(text):\n",
        "    sum = 0\n",
        "    for i in text:\n",
        "        if i in \"!$%&'()*+,-./:;<=>?[\\]^_`{|}~\":\n",
        "            sum +=1\n",
        "    return sum\n",
        "\n",
        "punctCount = udf(punctCount1)\n",
        "\n",
        "dataset_filtered = dataset_filtered.withColumn('PunctCount',punctCount(dataset_filtered['text']).astype('int'))\n",
        "def types_punctuation(text):\n",
        "    return len(set([i for i in text if i in \"!$%&'()*+,-./:;<=>?[\\]^_`{|}~\"]))\n",
        "\n",
        "typePunct = udf(types_punctuation)\n",
        "dataset_filtered = dataset_filtered.withColumn('typePunct',typePunct(dataset_filtered['text']).astype('int'))\n",
        "\n",
        "st.write(\"Ongoing feature extraction!! :heavy_check_mark:\")\n",
        "count = CountVectorizer(inputCol=\"wordss\", outputCol=\"rawFeatures\")\n",
        "count_model = count.fit(dataset_filtered)\n",
        "featurizedData = count_model.transform(dataset_filtered)\n",
        "\n",
        "def LabelEncoder(x):\n",
        "    if x == 'positive':\n",
        "        return 0\n",
        "    elif x == 'negative':\n",
        "        return 1\n",
        "    return 2\n",
        "encoder = udf(LabelEncoder)\n",
        "featurizedData = featurizedData.withColumn('label', encoder(featurizedData['airline_sentiment']).astype('int'))\n",
        "\n",
        "featureassembler = VectorAssembler(inputCols=['rawFeatures','airline_sentiment_confidence','negativereason_confidence',\n",
        "                                              'tweet_length','num_hashtags','num_mentions','retweet_count','PunctCount','typePunct']\n",
        "                                   ,outputCol='features')\n",
        "features = featureassembler.transform(featurizedData)\n",
        "\n",
        "final = features.withColumn('labels',featurizedData['label'])\n",
        "train_data,test_data = final.randomSplit([0.75,0.25])\n",
        "nb = NaiveBayes(featuresCol='features', labelCol='labels')\n",
        "st.write(\"Training the model :heavy_check_mark:\")\n",
        "\n",
        "model = nb.fit(train_data)\n",
        "\n",
        "def predict_users_Input(user_input):\n",
        "  df1 = spark.createDataFrame([ (1, user_input)],['Id', 'UserTweet'])\n",
        "\n",
        "  df1 = df1.withColumn('UserTweet',words(df1['UserTweet']))\n",
        "  df1 = df1.withColumn(\"wordss\", word_udf(\"UserTweet\"))\n",
        "  df1 = df1.withColumn(\"wordss\", punct_udf1(\"wordss\"))\n",
        "  df1 = df1.withColumn(\"wordss\", array_udf(\"wordss\"))\n",
        "  df1 = df1.withColumn(\"wordss\", lem(\"wordss\"))\n",
        "  df1 = df1.withColumn(\"wordss\", array_udf(\"wordss\"))\n",
        "\n",
        "  df1 = df1.withColumn('tweet_length', length(df1['UserTweet']).astype('int'))\n",
        "  df1 = df1.withColumn('num_hashtags', num_hashtags(df1['UserTweet']).astype('int'))\n",
        "  df1 = df1.withColumn('num_mentions', num_mentions(df1['UserTweet']).astype('int'))\n",
        "  df1 = df1.withColumn('PunctCount',punctCount(df1['UserTweet']).astype('int'))\n",
        "  df1 = df1.withColumn('typePunct',typePunct(df1['UserTweet']).astype('int'))\n",
        "  make  = udf(lambda x : 0)\n",
        "  df1 = df1.withColumn('negativereason_confidence',make(df1['UserTweet']).astype('int'))\n",
        "  df1 = df1.withColumn('airline_sentiment_confidence',make(df1['UserTweet']).astype('int'))\n",
        "  df1 = df1.withColumn('retweet_count',make(df1['UserTweet']).astype('int'))\n",
        "  df1_featured = count_model.transform(df1)\n",
        "  test_features = featureassembler.transform(df1_featured)\n",
        "  test_predict = model.transform(test_features)\n",
        "  df_res = test_predict.select('prediction').toPandas()\n",
        "  return df_res\n",
        "\n",
        "def decode(label):\n",
        "  if label == 0:\n",
        "    return \"Positive tweet!\"\n",
        "  elif label == 1:\n",
        "    return \"Negative Tweet!\"\n",
        "  return \"Neutral tweet\"\n",
        "\n",
        "user_input = st.text_input(\"Enter the text input\",\"Your tweet here \")\n",
        "if st.button('predict'):\n",
        "    result = predict_users_Input(user_input)\n",
        "    st.write(decode(result.prediction.values[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PvxEWsdN4Hao",
      "metadata": {
        "id": "PvxEWsdN4Hao"
      },
      "source": [
        "After you execute the code below you will get a web app link where you could perform the sentiment prediction task.\n",
        "\n",
        "The cell below keeps executing until the server is stopped by interrupting the execution. An error message may appear upon interruption, you could ignore it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Nz1JE6qv3JB8",
      "metadata": {
        "id": "Nz1JE6qv3JB8"
      },
      "outputs": [],
      "source": [
        "from colab_everything import ColabStreamlit\n",
        "ColabStreamlit('/content/app.py')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9TpfKaRvhkls",
      "metadata": {
        "id": "9TpfKaRvhkls"
      },
      "source": [
        "\n",
        "Refer the screenshot below.\n",
        "![img](https://cdn.iisc.talentsprint.com/CDS/MiniProjects/sentiment_analysis_streamlit_button.JPG)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
