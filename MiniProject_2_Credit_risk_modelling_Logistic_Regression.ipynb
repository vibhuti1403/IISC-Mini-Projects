{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "associate-sunset",
      "metadata": {
        "id": "associate-sunset"
      },
      "source": [
        "# Advanced Certification Program in Computational Data Science\n",
        "## A program by IISc and TalentSprint\n",
        "### Mini-Project: Credit risk modelling using Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "handled-tooth",
      "metadata": {
        "id": "handled-tooth"
      },
      "source": [
        "## Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "accessory-watts",
      "metadata": {
        "id": "accessory-watts"
      },
      "source": [
        "Predict the loan defaulters using a Logistic Regression model on the credit risk data and calculate credit scores"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "twenty-indonesia",
      "metadata": {
        "id": "twenty-indonesia"
      },
      "source": [
        "## Learning Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "honest-friendship",
      "metadata": {
        "id": "honest-friendship"
      },
      "source": [
        "At the end of the mini-project, you will be able to :\n",
        "\n",
        "* perform data exploration, preprocessing and visualization\n",
        "* implement Logistic Regression using manual code or using sklearn library\n",
        "* evaluate the model using appropriate performance metrics\n",
        "* develop a credit scoring system"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lesbian-bottom",
      "metadata": {
        "id": "lesbian-bottom"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fixed-trainer",
      "metadata": {
        "id": "fixed-trainer"
      },
      "source": [
        "The dataset chosen for this mini-project is the [Give Me Some Credit](https://bigml.com/user/jbosca/gallery/dataset/5a7def3d2a83476e09000456#info) dataset which can be used to build models for predicting loan repayment defaulters\n",
        "\n",
        "#### Datafields\n",
        "\n",
        "- **SeriousDlqin2yrs:** Person experienced 90 days past due delinquency or worse\n",
        "- **RevolvingUtilizationOfUnsecuredLines:** Total balance on credit cards and personal lines of credit except real estate and no installment debt like car loans divided by the sum of credit limits\n",
        "- **age:** Age of borrower in years\n",
        "- **NumberOfTime30-59DaysPastDueNotWorse:** Number of times borrower has been 30-59 days past due but no worse in the last 2 years.\n",
        "- **DebtRatio:** Monthly debt payments, alimony,living costs divided by monthy gross income\n",
        "- **MonthlyIncome:** Monthly income\n",
        "- **NumberOfOpenCreditLinesAndLoans:** Number of Open loans (installment like car loan or mortgage) and Lines of credit (e.g. credit cards)\n",
        "- **NumberOfTimes90DaysLate:** Number of times borrower has been 90 days or more past due.\n",
        "- **NumberRealEstateLoansOrLines:**\tNumber of mortgage and real estate loans including home equity lines of credit\n",
        "- **NumberOfTime60-89DaysPastDueNotWorse:**\tNumber of times borrower has been 60-89 days past due but no worse in the last 2 years.\n",
        "- **NumberOfDependents:** Number of dependents in family excluding themselves (spouse, children etc.)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rapid-hierarchy",
      "metadata": {
        "id": "rapid-hierarchy"
      },
      "source": [
        "## Information"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "prescribed-matter",
      "metadata": {
        "id": "prescribed-matter"
      },
      "source": [
        "Credit risk arises when a corporate or individual borrower fails to meet their debt obligations. From the lender's perspective, credit risk could disrupt its cash flows or increase collection costs, since the lender may be forced to hire a debt collection agency to enforce the collection. The loss may be partial or complete, where the lender incurs a loss of part of the loan or the entire loan extended to the borrower.\n",
        "\n",
        "Credit scoring algorithms, which calculate the probability of default, are the best methods that banks use to determine whether or not a loan should be granted.\n",
        "\n",
        "In order to build a credit scoring system, the following feature transformations are performed:\n",
        "\n",
        "#### Weight of Evidence and Information value\n",
        "\n",
        "Logistic regression is a commonly used technique in credit scoring for solving binary classification problems. Prior to model fitting, another iteration of variable selection is valuable to check if the newly WOE transformed variables are still good model candidates. Preferred candidate variables are those with higher information value having a linear relationship with the dependent variable, have good coverage across all categories, have a normal distribution, contain a notable overall contribution, and are relevant to the business.\n",
        "\n",
        "**Weight of evidence** (WOE) is a powerful tool for feature representation and evaluation in data science. WOE can provide interpret able transformation to both categorical and numerical features. The weight of evidence tells the predictive power of an independent variable in relation to the dependent variable. Since it evolved from credit scoring world, it is generally described as a measure of the separation of good and bad customers. \"Bad Customers\" refers to the customers who defaulted on a loan. and \"Good Customers\" refers to the customers who paid back loan. WOE can be calculated using the below formula:\n",
        "\n",
        "$$WOE = ln \\left( \\frac{\\%   of  Non\\_Events}{\\%   of  Events} \\right)$$\n",
        "\n",
        "Steps to calculate WOE\n",
        "* For a continuous variable, split data into 10 parts (or lesser depending on the distribution).\n",
        "* Calculate the number of events and non-events in each group (bin)\n",
        "* Calculate the % of events and % of non-events in each group.\n",
        "* Calculate WOE by taking natural log of division of % of non-events and % of events\n",
        "\n",
        "**Information value** is one of the most useful technique to select important variables in a predictive model. It helps to rank variables on the basis of their importance. The IV is calculated using the following formula :\n",
        "$$IV = ∑ (\\% of Non\\_Events - \\% of Events) * WOE$$\n",
        "\n",
        "Read more about `WOE` and `IV` from the following [link](https://medium.com/@yanhuiliu104/credit-scoring-scorecard-development-process-8554c3492b2b)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "operating-latter",
      "metadata": {
        "id": "operating-latter"
      },
      "source": [
        "## Grading = 10 Points"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "caring-syndrome",
      "metadata": {
        "id": "caring-syndrome"
      },
      "source": [
        "### Download the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "comparable-delay",
      "metadata": {
        "id": "comparable-delay"
      },
      "outputs": [],
      "source": [
        "!wget -qq https://cdn.iisc.talentsprint.com/CDS/MiniProjects/GiveMeSomeCredit.csv\n",
        "!pip -qq install xverse"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "appreciated-pattern",
      "metadata": {
        "id": "appreciated-pattern"
      },
      "source": [
        "### Import Neccesary Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "loose-marsh",
      "metadata": {
        "id": "loose-marsh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "import math\n",
        "from xverse.transformer import MonotonicBinning,WOE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "compressed-reflection",
      "metadata": {
        "id": "compressed-reflection"
      },
      "source": [
        "### Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fatty-graph",
      "metadata": {
        "id": "fatty-graph"
      },
      "outputs": [],
      "source": [
        "train_data = pd.read_csv(\"GiveMeSomeCredit.csv\")\n",
        "train_data.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "experienced-sleeping",
      "metadata": {
        "id": "experienced-sleeping"
      },
      "source": [
        "#### Describe the all statistical properties of the train dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "greek-methodology",
      "metadata": {
        "id": "greek-methodology"
      },
      "outputs": [],
      "source": [
        "train_data[train_data.columns[1:]].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "christian-hamilton",
      "metadata": {
        "id": "christian-hamilton"
      },
      "source": [
        "### Pre-processing (1 point)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "global-decision",
      "metadata": {
        "id": "global-decision"
      },
      "source": [
        "#### Remove unwanted columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pharmaceutical-latvia",
      "metadata": {
        "id": "pharmaceutical-latvia"
      },
      "outputs": [],
      "source": [
        "train_data.drop(\"Unnamed: 0\",axis=1,inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "usual-elimination",
      "metadata": {
        "id": "usual-elimination"
      },
      "source": [
        "#### Handle the missing data\n",
        "\n",
        "Find the how many null values in the dataset and fill with mean or remove."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "standing-trinity",
      "metadata": {
        "id": "standing-trinity"
      },
      "outputs": [],
      "source": [
        "train_data.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "heated-findings",
      "metadata": {
        "id": "heated-findings"
      },
      "outputs": [],
      "source": [
        "# Fill the missing values using mean\n",
        "train_data= train_data.fillna((train_data.mean()))\n",
        "train_data.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hispanic-reply",
      "metadata": {
        "id": "hispanic-reply"
      },
      "source": [
        "### EDA &  Visualization ( 1 point)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "standing-cheese",
      "metadata": {
        "id": "standing-cheese"
      },
      "source": [
        "#### Calculate the percentage of the target lebels and visualize with a graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "attractive-hands",
      "metadata": {
        "id": "attractive-hands"
      },
      "outputs": [],
      "source": [
        "total_len = len(train_data['SeriousDlqin2yrs'])\n",
        "percentage_labels = (train_data['SeriousDlqin2yrs'].value_counts()/total_len)*100\n",
        "percentage_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cutting-citizenship",
      "metadata": {
        "id": "cutting-citizenship"
      },
      "outputs": [],
      "source": [
        "sns.countplot(train_data.SeriousDlqin2yrs).set_title('Data Distribution')\n",
        "ax = plt.gca()\n",
        "for p in ax.patches:\n",
        "    height = p.get_height()\n",
        "    ax.text(p.get_x() + p.get_width()/2., height + 2, '{:.2f}%'.format(100*(height/total_len)), fontsize=14, ha='center', va='bottom')\n",
        "ax.set_xlabel(\"Labels for SeriousDlqin2yrs attribute\")\n",
        "ax.set_ylabel(\"Numbers of records\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "satisfactory-stopping",
      "metadata": {
        "id": "satisfactory-stopping"
      },
      "source": [
        "#### Plot the distribution of SeriousDlqin2yrs by age"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "multiple-series",
      "metadata": {
        "id": "multiple-series"
      },
      "outputs": [],
      "source": [
        "sns.kdeplot(train_data[train_data[\"SeriousDlqin2yrs\"] == 0][\"age\"], label=\"Not in 2 years\")\n",
        "sns.kdeplot(train_data[train_data[\"SeriousDlqin2yrs\"] == 1][\"age\"], label=\"In 2 years\")\n",
        "plt.xlabel('Age')\n",
        "plt.title('Distribuition of Default Rate by Age')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "promotional-rolling",
      "metadata": {
        "id": "promotional-rolling"
      },
      "source": [
        "#### Correlation and the heatmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "studied-candidate",
      "metadata": {
        "id": "studied-candidate"
      },
      "outputs": [],
      "source": [
        "train_data[train_data.columns[:]].corr()\n",
        "sns.heatmap(train_data[train_data.columns[:]].corr(),fmt=\".1f\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "operational-minute",
      "metadata": {
        "id": "operational-minute"
      },
      "source": [
        "### Data Engineering (2 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "outer-telephone",
      "metadata": {
        "id": "outer-telephone"
      },
      "source": [
        "#### Weight of Evidence and Information value\n",
        "\n",
        "* Arrange the binning for each variable with different bins\n",
        "    * For eg. Age = 49, Age_quantile_range = (48, 56)\n",
        "* Calculate information value and chooose the best features based on the rules given below\n",
        "\n",
        "| Information Value |\tVariable Predictiveness |\n",
        "| --- | --- |\n",
        "| Less than 0.02\t|  Not useful for prediction |\n",
        "| 0.02 to 0.1\t| Weak predictive Power |\n",
        "|  0.1 to 0.3 | Medium predictive Power |\n",
        "| 0.3 to 0.5 | Strong predictive Power |\n",
        "| >0.5 | Suspicious Predictive Power |\n",
        "\n",
        "* Calculate Weight of evidence for the selected variables\n",
        "\n",
        "Hint: Use [xverse](https://towardsdatascience.com/introducing-xverse-a-python-package-for-feature-selection-and-transformation-17193cdcd067). It is a machine learning Python module in the space of feature engineering, feature transformation and feature selection. It provides pre-built functions for the above steps, such as binning and conversion to WoE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "norwegian-telescope",
      "metadata": {
        "id": "norwegian-telescope"
      },
      "outputs": [],
      "source": [
        "# Using xverse package\n",
        "clf = MonotonicBinning()\n",
        "clf.fit(train_data.iloc[:,1:], train_data.iloc[:,0])\n",
        "out_X = clf.transform(train_data.iloc[:,1:])\n",
        "out_X.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "liked-satin",
      "metadata": {
        "id": "liked-satin"
      },
      "outputs": [],
      "source": [
        "# Using xverse package\n",
        "clf = WOE()\n",
        "clf.fit(train_data.iloc[:,1:], train_data.iloc[:,0])\n",
        "out_X = clf.transform(train_data.iloc[:,1:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "charitable-anxiety",
      "metadata": {
        "id": "charitable-anxiety"
      },
      "outputs": [],
      "source": [
        "clf.iv_df #information value dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ordered-knock",
      "metadata": {
        "id": "ordered-knock"
      },
      "outputs": [],
      "source": [
        "selected_columns = clf.iv_df.Variable_Name.values[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "conservative-rebel",
      "metadata": {
        "id": "conservative-rebel"
      },
      "source": [
        "### Identify features,  target and split it into train and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ambient-dress",
      "metadata": {
        "id": "ambient-dress"
      },
      "outputs": [],
      "source": [
        "len(out_X.columns), len(selected_columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5hoEeUh66UeT",
      "metadata": {
        "id": "5hoEeUh66UeT"
      },
      "outputs": [],
      "source": [
        "train_data.iloc[:,1:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "harmful-warrant",
      "metadata": {
        "id": "harmful-warrant"
      },
      "outputs": [],
      "source": [
        "X = out_X[selected_columns]\n",
        "y = train_data['SeriousDlqin2yrs']\n",
        "X.shape, y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "packed-humor",
      "metadata": {
        "id": "packed-humor"
      },
      "outputs": [],
      "source": [
        "# split the data into train and test\n",
        "xtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=66)\n",
        "xtrain.shape, ytrain.shape, xtest.shape, ytest.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ufESq5OxgoXa",
      "metadata": {
        "id": "ufESq5OxgoXa"
      },
      "source": [
        "### Logistic Regression from scratch using gradient method (2 points)\n",
        "\n",
        "For Linear Regression, we had the hypothesis $yhat = w.X +b$ , whose output range was the set of all Real Numbers.\n",
        "Now, for Logistic Regression our hypothesis is  $yhat = sigmoid(w.X + b)$ , whose output range is between 0 and 1 because by applying a sigmoid function, we always output a number between 0 and 1.\n",
        "\n",
        "$yhat = \\frac{1}{1 +e^{-(w.x+b)}}$\n",
        "\n",
        "Hint: [logistic-regression-with-python](\n",
        "https://medium.com/@ODSC/logistic-regression-with-python-ede39f8573c7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "precious-business",
      "metadata": {
        "id": "precious-business"
      },
      "outputs": [],
      "source": [
        "intercept = np.ones((xtrain.shape[0], 1))\n",
        "x_train = np.concatenate((intercept, xtrain), axis=1)\n",
        "weight = np.zeros(x_train.shape[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "confidential-curtis",
      "metadata": {
        "id": "confidential-curtis"
      },
      "outputs": [],
      "source": [
        "weight.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "middle-mailing",
      "metadata": {
        "id": "middle-mailing"
      },
      "outputs": [],
      "source": [
        "#Sigmoid method\n",
        "def sigmoid(x, weight):\n",
        "    z = np.dot(x, weight)\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "def fit(x, y, weight, lr, iterations):\n",
        "    for i in range(iterations):\n",
        "        sigma = sigmoid(x, weight)\n",
        "        loss = mean_squared_error(sigma,y)\n",
        "        # gradient\n",
        "        dW = np.dot(x.T, (sigma - y)) / y.shape[0]\n",
        "        #Updating the weights\n",
        "        weight -= lr * dW\n",
        "    return weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "quick-study",
      "metadata": {
        "id": "quick-study"
      },
      "outputs": [],
      "source": [
        "#creating the class Object\n",
        "updated_weights = fit(x_train, ytrain, weight, 0.1 , 5000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vertical-layout",
      "metadata": {
        "id": "vertical-layout"
      },
      "outputs": [],
      "source": [
        "# Method to predict the class label.\n",
        "def predict(x_new , weight, treshold):\n",
        "    x_new = np.concatenate((np.ones((x_new.shape[0], 1)), x_new), axis=1)\n",
        "    result = sigmoid(x_new, weight)\n",
        "    result = result >= treshold\n",
        "    y_pred = np.zeros(result.shape[0])\n",
        "    for i in range(len(y_pred)):\n",
        "        if result[i] == True:\n",
        "            y_pred[i] = 1\n",
        "        else:\n",
        "            continue\n",
        "    return y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tracked-intent",
      "metadata": {
        "id": "tracked-intent"
      },
      "outputs": [],
      "source": [
        "xtest.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bigger-guidance",
      "metadata": {
        "id": "bigger-guidance"
      },
      "outputs": [],
      "source": [
        "y_pred = predict(xtest, updated_weights, 0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lasting-sector",
      "metadata": {
        "id": "lasting-sector"
      },
      "outputs": [],
      "source": [
        "# Accuracy of test data\n",
        "(y_pred == ytest).sum() / len(y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7eSQ5nuBgk7S",
      "metadata": {
        "id": "7eSQ5nuBgk7S"
      },
      "source": [
        "### Implement the Logistic regression using sklearn (2 points)\n",
        "\n",
        "As there is imbalance in the class distribution, add weightage to the Logistic regression.\n",
        "\n",
        "* Find the accuracy with class weightage in Logistic regression\n",
        "* Find the accuracy without class weightage in Logistic regression\n",
        "\n",
        "Hint: [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "impressive-assistant",
      "metadata": {
        "id": "impressive-assistant"
      },
      "outputs": [],
      "source": [
        "# With weightage\n",
        "log_reg = LogisticRegression(class_weight={0:6/100, 1: 94/100})\n",
        "log_reg.fit(xtrain,ytrain)\n",
        "log_reg.score(xtest,ytest), log_reg.score(xtrain, ytrain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AFz6VF-m5roR",
      "metadata": {
        "id": "AFz6VF-m5roR"
      },
      "outputs": [],
      "source": [
        "test_predicted = log_reg.predict(xtest)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "similar-flower",
      "metadata": {
        "id": "similar-flower"
      },
      "outputs": [],
      "source": [
        "# Without weightage\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(xtrain,ytrain)\n",
        "log_reg.score(xtest,ytest), log_reg.score(xtrain, ytrain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fossil-washer",
      "metadata": {
        "id": "fossil-washer"
      },
      "outputs": [],
      "source": [
        "test_predicted = log_reg.predict(xtest)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "heard-briefing",
      "metadata": {
        "id": "heard-briefing"
      },
      "outputs": [],
      "source": [
        "log_reg.coef_, log_reg.intercept_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "q68_0W3G9jCn",
      "metadata": {
        "id": "q68_0W3G9jCn"
      },
      "outputs": [],
      "source": [
        "log_reg.predict_proba"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "signal-error",
      "metadata": {
        "id": "signal-error"
      },
      "source": [
        "### Credit scoring (1 point)\n",
        "\n",
        "When scaling the model into a scorecard, we will need both the Logistic Regression coefficients from model fitting as well as the transformed WoE values. We will also need to convert the score from the model from the log-odds unit to a points system.\n",
        "For each independent variable Xi, its corresponding score is:\n",
        "\n",
        "$Score = \\sum_{i=1}^{n} (-(β_i × WoE_i + \\frac{α}{n}) × Factor + \\frac{Offset}{n})$\n",
        "\n",
        "Where:\n",
        "\n",
        "βi — logistic regression coefficient for the variable Xi\n",
        "\n",
        "α — logistic regression intercept\n",
        "\n",
        "WoE — Weight of Evidence value for variable Xi\n",
        "\n",
        "n — number of independent variable Xi in the model\n",
        "\n",
        "Factor, Offset — known as scaling parameter\n",
        "\n",
        "  - Factor = pdo / ln(2); pdo is points to double the odds\n",
        "  - Offset = Round_of_Score - {Factor * ln(Odds)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "worst-spare",
      "metadata": {
        "id": "worst-spare"
      },
      "outputs": [],
      "source": [
        "coef = log_reg.coef_.ravel()\n",
        "intercept = log_reg.intercept_\n",
        "factor = 20/np.log(2)\n",
        "offset = 600 - ( factor * np.log(50))\n",
        "factor, offset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MwSXbZ6_P8Jr",
      "metadata": {
        "id": "MwSXbZ6_P8Jr"
      },
      "outputs": [],
      "source": [
        "# 1st method\n",
        "# all_scores = []\n",
        "# for idx,row in X.iterrows():\n",
        "#   score  = []\n",
        "#   for j in range(len(row)):\n",
        "#     asum = (-((row[j] * coef[j]) + (intercept/X.shape[1])) * factor) + (offset/X.shape[1])\n",
        "#     score.append(asum)\n",
        "#   all_scores.append(sum(score))\n",
        "# max(all_scores), min(all_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XwwnwQKMU_Nx",
      "metadata": {
        "id": "XwwnwQKMU_Nx"
      },
      "outputs": [],
      "source": [
        "# 2nd method\n",
        "all_scores = []\n",
        "for idx,row in X.iterrows():\n",
        "  a = row.values * coef          # B_i * WOE_i\n",
        "  a = a + (intercept/X.shape[1]) # (B_i * WOE_i) + intercept_i / n\n",
        "  b = -a * factor                # -((B_i * WOE_i) + intercept_i / n) * factor\n",
        "  b = b + (offset/X.shape[1])    # -((B_i * WOE_i) + intercept_i / n) * factor) + offset / n\n",
        "  all_scores.append(sum(b))      # sum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vOcB1ewAYxtM",
      "metadata": {
        "id": "vOcB1ewAYxtM"
      },
      "outputs": [],
      "source": [
        "max(all_scores),min(all_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "intelligent-internship",
      "metadata": {
        "id": "intelligent-internship"
      },
      "source": [
        "### Performance Metrics (1 point)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "innocent-hygiene",
      "metadata": {
        "id": "innocent-hygiene"
      },
      "source": [
        "#### Precision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "optimum-listening",
      "metadata": {
        "id": "optimum-listening"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score\n",
        "precision_score(ytest, test_predicted ,average='macro')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "accessory-keyboard",
      "metadata": {
        "id": "accessory-keyboard"
      },
      "source": [
        "#### Recall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "civic-corner",
      "metadata": {
        "id": "civic-corner"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import recall_score\n",
        "recall_score(ytest, test_predicted,average='macro')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wired-amendment",
      "metadata": {
        "id": "wired-amendment"
      },
      "source": [
        "#### Classification Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "impossible-machinery",
      "metadata": {
        "id": "impossible-machinery"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(ytest, test_predicted))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dense-feelings",
      "metadata": {
        "id": "dense-feelings"
      },
      "source": [
        "#### Confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "running-remains",
      "metadata": {
        "id": "running-remains"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "mat = confusion_matrix(ytest, test_predicted)\n",
        "mat"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
